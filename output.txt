CUDA compat package requires Nvidia driver â©½570.172.08
Current installed Nvidia driver version is 570.172.08
Skip CUDA compat libs setup as newer Nvidia driver is installed
LMI_DEBUG_NSYS_ENABLED=
/opt/djl/bin/telemetry.sh: line 35: /opt/djl/logs/telemetry-test: Permission denied
2025-09-23T05:12:59.959078200Z main ERROR RollingFileManager (/opt/djl/logs/serving.log) java.io.FileNotFoundException: /opt/djl/logs/serving.log (Permission denied) java.io.FileNotFoundException: /opt/djl/logs/serving.log (Permission denied)
	at java.base/java.io.FileOutputStream.open0(Native Method)
	at java.base/java.io.FileOutputStream.open(FileOutputStream.java:293)
	at java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:235)
	at java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:155)
	at org.apache.logging.log4j.core.appender.rolling.RollingFileManager$RollingFileManagerFactory.createManager(RollingFileManager.java:878)
	at org.apache.logging.log4j.core.appender.rolling.RollingFileManager$RollingFileManagerFactory.createManager(RollingFileManager.java:844)
	at org.apache.logging.log4j.core.appender.AbstractManager.getManager(AbstractManager.java:146)
	at org.apache.logging.log4j.core.appender.OutputStreamManager.getManager(OutputStreamManager.java:112)
	at org.apache.logging.log4j.core.appender.rolling.RollingFileManager.getFileManager(RollingFileManager.java:307)
	at org.apache.logging.log4j.core.appender.RollingFileAppender$Builder.build(RollingFileAppender.java:144)
	at org.apache.logging.log4j.core.appender.RollingFileAppender$Builder.build(RollingFileAppender.java:65)
	at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:124)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:1180)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1099)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1091)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:695)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:270)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:319)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:673)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:762)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:784)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:300)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:46)
	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:138)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:136)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:58)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:46)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:32)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:447)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:472)
	at ai.djl.serving.ModelServer.<clinit>(ModelServer.java:69)

2025-09-23T05:12:59.966945859Z main ERROR Could not create plugin of type class org.apache.logging.log4j.core.appender.RollingFileAppender for element RollingFile: java.lang.IllegalStateException: ManagerFactory [org.apache.logging.log4j.core.appender.rolling.RollingFileManager$RollingFileManagerFactory@7f3b84b8] unable to create manager for [/opt/djl/logs/serving.log] with data [org.apache.logging.log4j.core.appender.rolling.RollingFileManager$FactoryData@57a3af25[pattern=/opt/djl/logs/serving.%d{dd-MMM}-%i.log.gz, append=true, bufferedIO=true, bufferSize=8192, policy=CompositeTriggeringPolicy(policies=[OnStartupTriggeringPolicy, SizeBasedTriggeringPolicy(size=10485760), TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=false)]), strategy=DefaultRolloverStrategy(min=1, max=5, useMax=true), advertiseURI=null, layout=[%-5level] %d{yyyy-MM-dd HH:mm:ss} %c{1} - %m%n, filePermissions=null, fileOwner=null]] java.lang.IllegalStateException: ManagerFactory [org.apache.logging.log4j.core.appender.rolling.RollingFileManager$RollingFileManagerFactory@7f3b84b8] unable to create manager for [/opt/djl/logs/serving.log] with data [org.apache.logging.log4j.core.appender.rolling.RollingFileManager$FactoryData@57a3af25[pattern=/opt/djl/logs/serving.%d{dd-MMM}-%i.log.gz, append=true, bufferedIO=true, bufferSize=8192, policy=CompositeTriggeringPolicy(policies=[OnStartupTriggeringPolicy, SizeBasedTriggeringPolicy(size=10485760), TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=false)]), strategy=DefaultRolloverStrategy(min=1, max=5, useMax=true), advertiseURI=null, layout=[%-5level] %d{yyyy-MM-dd HH:mm:ss} %c{1} - %m%n, filePermissions=null, fileOwner=null]]
	at org.apache.logging.log4j.core.appender.AbstractManager.getManager(AbstractManager.java:148)
	at org.apache.logging.log4j.core.appender.OutputStreamManager.getManager(OutputStreamManager.java:112)
	at org.apache.logging.log4j.core.appender.rolling.RollingFileManager.getFileManager(RollingFileManager.java:307)
	at org.apache.logging.log4j.core.appender.RollingFileAppender$Builder.build(RollingFileAppender.java:144)
	at org.apache.logging.log4j.core.appender.RollingFileAppender$Builder.build(RollingFileAppender.java:65)
	at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:124)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:1180)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1099)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1091)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:695)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:270)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:319)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:673)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:762)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:784)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:300)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:46)
	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:138)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:136)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:58)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:46)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:32)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:447)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:472)
	at ai.djl.serving.ModelServer.<clinit>(ModelServer.java:69)

2025-09-23T05:12:59.968646558Z main ERROR Unable to invoke factory method in class org.apache.logging.log4j.core.appender.RollingFileAppender for element RollingFile: java.lang.IllegalStateException: No factory method found for class org.apache.logging.log4j.core.appender.RollingFileAppender java.lang.IllegalStateException: No factory method found for class org.apache.logging.log4j.core.appender.RollingFileAppender
	at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.findFactoryMethod(PluginBuilder.java:268)
	at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:140)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:1180)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1099)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1091)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:695)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:270)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:319)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:673)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:762)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:784)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:300)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:46)
	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:138)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:136)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:58)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:46)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:32)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:447)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:472)
	at ai.djl.serving.ModelServer.<clinit>(ModelServer.java:69)

2025-09-23T05:12:59.971495110Z main ERROR Unable to create file /opt/djl/logs/access.log java.io.IOException: Permission denied
	at java.base/java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.base/java.io.File.createNewFile(File.java:1043)
	at org.apache.logging.log4j.core.appender.rolling.RollingFileManager$RollingFileManagerFactory.createManager(RollingFileManager.java:864)
	at org.apache.logging.log4j.core.appender.rolling.RollingFileManager$RollingFileManagerFactory.createManager(RollingFileManager.java:844)
	at org.apache.logging.log4j.core.appender.AbstractManager.getManager(AbstractManager.java:146)
	at org.apache.logging.log4j.core.appender.OutputStreamManager.getManager(OutputStreamManager.java:112)
	at org.apache.logging.log4j.core.appender.rolling.RollingFileManager.getFileManager(RollingFileManager.java:307)
	at org.apache.logging.log4j.core.appender.RollingFileAppender$Builder.build(RollingFileAppender.java:144)
	at org.apache.logging.log4j.core.appender.RollingFileAppender$Builder.build(RollingFileAppender.java:65)
	at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:124)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:1180)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1099)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1091)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:695)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:270)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:319)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:673)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:762)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:784)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:300)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:46)
	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:138)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:136)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:58)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:46)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:32)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:447)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:472)
	at ai.djl.serving.ModelServer.<clinit>(ModelServer.java:69)

2025-09-23T05:12:59.972254909Z main ERROR Could not create plugin of type class org.apache.logging.log4j.core.appender.RollingFileAppender for element RollingFile: java.lang.IllegalStateException: ManagerFactory [org.apache.logging.log4j.core.appender.rolling.RollingFileManager$RollingFileManagerFactory@7f3b84b8] unable to create manager for [/opt/djl/logs/access.log] with data [org.apache.logging.log4j.core.appender.rolling.RollingFileManager$FactoryData@1283bb96[pattern=/opt/djl/logs/access.%d{dd-MMM}-%i.log.gz, append=true, bufferedIO=true, bufferSize=8192, policy=CompositeTriggeringPolicy(policies=[OnStartupTriggeringPolicy, SizeBasedTriggeringPolicy(size=10485760), TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=false)]), strategy=DefaultRolloverStrategy(min=1, max=5, useMax=true), advertiseURI=null, layout=%d{yyyy-MM-dd HH:mm:ss} %m%n, filePermissions=null, fileOwner=null]] java.lang.IllegalStateException: ManagerFactory [org.apache.logging.log4j.core.appender.rolling.RollingFileManager$RollingFileManagerFactory@7f3b84b8] unable to create manager for [/opt/djl/logs/access.log] with data [org.apache.logging.log4j.core.appender.rolling.RollingFileManager$FactoryData@1283bb96[pattern=/opt/djl/logs/access.%d{dd-MMM}-%i.log.gz, append=true, bufferedIO=true, bufferSize=8192, policy=CompositeTriggeringPolicy(policies=[OnStartupTriggeringPolicy, SizeBasedTriggeringPolicy(size=10485760), TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=false)]), strategy=DefaultRolloverStrategy(min=1, max=5, useMax=true), advertiseURI=null, layout=%d{yyyy-MM-dd HH:mm:ss} %m%n, filePermissions=null, fileOwner=null]]
	at org.apache.logging.log4j.core.appender.AbstractManager.getManager(AbstractManager.java:148)
	at org.apache.logging.log4j.core.appender.OutputStreamManager.getManager(OutputStreamManager.java:112)
	at org.apache.logging.log4j.core.appender.rolling.RollingFileManager.getFileManager(RollingFileManager.java:307)
	at org.apache.logging.log4j.core.appender.RollingFileAppender$Builder.build(RollingFileAppender.java:144)
	at org.apache.logging.log4j.core.appender.RollingFileAppender$Builder.build(RollingFileAppender.java:65)
	at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:124)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:1180)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1099)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1091)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:695)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:270)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:319)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:673)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:762)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:784)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:300)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:46)
	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:138)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:136)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:58)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:46)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:32)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:447)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:472)
	at ai.djl.serving.ModelServer.<clinit>(ModelServer.java:69)

2025-09-23T05:12:59.972880767Z main ERROR Unable to invoke factory method in class org.apache.logging.log4j.core.appender.RollingFileAppender for element RollingFile: java.lang.IllegalStateException: No factory method found for class org.apache.logging.log4j.core.appender.RollingFileAppender java.lang.IllegalStateException: No factory method found for class org.apache.logging.log4j.core.appender.RollingFileAppender
	at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.findFactoryMethod(PluginBuilder.java:268)
	at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:140)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:1180)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1099)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1091)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:695)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:270)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:319)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:673)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:762)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:784)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:300)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:46)
	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:138)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:136)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:58)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:46)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:32)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:447)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:472)
	at ai.djl.serving.ModelServer.<clinit>(ModelServer.java:69)

2025-09-23T05:12:59.974833094Z main ERROR Unable to create file /opt/djl/logs/server_metric.log java.io.IOException: Permission denied
	at java.base/java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.base/java.io.File.createNewFile(File.java:1043)
	at org.apache.logging.log4j.core.appender.rolling.RollingFileManager$RollingFileManagerFactory.createManager(RollingFileManager.java:864)
	at org.apache.logging.log4j.core.appender.rolling.RollingFileManager$RollingFileManagerFactory.createManager(RollingFileManager.java:844)
	at org.apache.logging.log4j.core.appender.AbstractManager.getManager(AbstractManager.java:146)
	at org.apache.logging.log4j.core.appender.OutputStreamManager.getManager(OutputStreamManager.java:112)
	at org.apache.logging.log4j.core.appender.rolling.RollingFileManager.getFileManager(RollingFileManager.java:307)
	at org.apache.logging.log4j.core.appender.RollingFileAppender$Builder.build(RollingFileAppender.java:144)
	at org.apache.logging.log4j.core.appender.RollingFileAppender$Builder.build(RollingFileAppender.java:65)
	at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:124)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:1180)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1099)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1091)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:695)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:270)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:319)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:673)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:762)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:784)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:300)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:46)
	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:138)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:136)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:58)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:46)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:32)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:447)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:472)
	at ai.djl.serving.ModelServer.<clinit>(ModelServer.java:69)

2025-09-23T05:12:59.975349926Z main ERROR Could not create plugin of type class org.apache.logging.log4j.core.appender.RollingFileAppender for element RollingFile: java.lang.IllegalStateException: ManagerFactory [org.apache.logging.log4j.core.appender.rolling.RollingFileManager$RollingFileManagerFactory@7f3b84b8] unable to create manager for [/opt/djl/logs/server_metric.log] with data [org.apache.logging.log4j.core.appender.rolling.RollingFileManager$FactoryData@3349e9bb[pattern=/opt/djl/logs/server_metric.%d{dd-MMM}-%i.log.gz, append=true, bufferedIO=true, bufferSize=8192, policy=CompositeTriggeringPolicy(policies=[OnStartupTriggeringPolicy, SizeBasedTriggeringPolicy(size=10485760), TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=false)]), strategy=DefaultRolloverStrategy(min=1, max=5, useMax=true), advertiseURI=null, layout=%d{yyyy-MM-dd HH:mm:ss} %m%n, filePermissions=null, fileOwner=null]] java.lang.IllegalStateException: ManagerFactory [org.apache.logging.log4j.core.appender.rolling.RollingFileManager$RollingFileManagerFactory@7f3b84b8] unable to create manager for [/opt/djl/logs/server_metric.log] with data [org.apache.logging.log4j.core.appender.rolling.RollingFileManager$FactoryData@3349e9bb[pattern=/opt/djl/logs/server_metric.%d{dd-MMM}-%i.log.gz, append=true, bufferedIO=true, bufferSize=8192, policy=CompositeTriggeringPolicy(policies=[OnStartupTriggeringPolicy, SizeBasedTriggeringPolicy(size=10485760), TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=false)]), strategy=DefaultRolloverStrategy(min=1, max=5, useMax=true), advertiseURI=null, layout=%d{yyyy-MM-dd HH:mm:ss} %m%n, filePermissions=null, fileOwner=null]]
	at org.apache.logging.log4j.core.appender.AbstractManager.getManager(AbstractManager.java:148)
	at org.apache.logging.log4j.core.appender.OutputStreamManager.getManager(OutputStreamManager.java:112)
	at org.apache.logging.log4j.core.appender.rolling.RollingFileManager.getFileManager(RollingFileManager.java:307)
	at org.apache.logging.log4j.core.appender.RollingFileAppender$Builder.build(RollingFileAppender.java:144)
	at org.apache.logging.log4j.core.appender.RollingFileAppender$Builder.build(RollingFileAppender.java:65)
	at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:124)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:1180)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1099)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1091)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:695)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:270)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:319)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:673)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:762)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:784)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:300)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:46)
	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:138)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:136)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:58)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:46)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:32)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:447)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:472)
	at ai.djl.serving.ModelServer.<clinit>(ModelServer.java:69)

2025-09-23T05:12:59.975915172Z main ERROR Unable to invoke factory method in class org.apache.logging.log4j.core.appender.RollingFileAppender for element RollingFile: java.lang.IllegalStateException: No factory method found for class org.apache.logging.log4j.core.appender.RollingFileAppender java.lang.IllegalStateException: No factory method found for class org.apache.logging.log4j.core.appender.RollingFileAppender
	at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.findFactoryMethod(PluginBuilder.java:268)
	at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:140)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:1180)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1099)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1091)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:695)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:270)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:319)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:673)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:762)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:784)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:300)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:46)
	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:138)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:136)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:58)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:46)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:32)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:447)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:472)
	at ai.djl.serving.ModelServer.<clinit>(ModelServer.java:69)

2025-09-23T05:12:59.977687Z main ERROR Unable to create file /opt/djl/logs/model_metric.log java.io.IOException: Permission denied
	at java.base/java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.base/java.io.File.createNewFile(File.java:1043)
	at org.apache.logging.log4j.core.appender.rolling.RollingFileManager$RollingFileManagerFactory.createManager(RollingFileManager.java:864)
	at org.apache.logging.log4j.core.appender.rolling.RollingFileManager$RollingFileManagerFactory.createManager(RollingFileManager.java:844)
	at org.apache.logging.log4j.core.appender.AbstractManager.getManager(AbstractManager.java:146)
	at org.apache.logging.log4j.core.appender.OutputStreamManager.getManager(OutputStreamManager.java:112)
	at org.apache.logging.log4j.core.appender.rolling.RollingFileManager.getFileManager(RollingFileManager.java:307)
	at org.apache.logging.log4j.core.appender.RollingFileAppender$Builder.build(RollingFileAppender.java:144)
	at org.apache.logging.log4j.core.appender.RollingFileAppender$Builder.build(RollingFileAppender.java:65)
	at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:124)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:1180)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1099)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1091)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:695)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:270)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:319)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:673)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:762)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:784)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:300)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:46)
	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:138)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:136)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:58)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:46)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:32)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:447)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:472)
	at ai.djl.serving.ModelServer.<clinit>(ModelServer.java:69)

2025-09-23T05:12:59.978334893Z main ERROR Could not create plugin of type class org.apache.logging.log4j.core.appender.RollingFileAppender for element RollingFile: java.lang.IllegalStateException: ManagerFactory [org.apache.logging.log4j.core.appender.rolling.RollingFileManager$RollingFileManagerFactory@7f3b84b8] unable to create manager for [/opt/djl/logs/model_metric.log] with data [org.apache.logging.log4j.core.appender.rolling.RollingFileManager$FactoryData@351d00c0[pattern=/opt/djl/logs/model_metric.%d{dd-MMM}-%i.log.gz, append=true, bufferedIO=true, bufferSize=8192, policy=CompositeTriggeringPolicy(policies=[OnStartupTriggeringPolicy, SizeBasedTriggeringPolicy(size=10485760), TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=false)]), strategy=DefaultRolloverStrategy(min=1, max=5, useMax=true), advertiseURI=null, layout=%d{yyyy-MM-dd HH:mm:ss} %m%n, filePermissions=null, fileOwner=null]] java.lang.IllegalStateException: ManagerFactory [org.apache.logging.log4j.core.appender.rolling.RollingFileManager$RollingFileManagerFactory@7f3b84b8] unable to create manager for [/opt/djl/logs/model_metric.log] with data [org.apache.logging.log4j.core.appender.rolling.RollingFileManager$FactoryData@351d00c0[pattern=/opt/djl/logs/model_metric.%d{dd-MMM}-%i.log.gz, append=true, bufferedIO=true, bufferSize=8192, policy=CompositeTriggeringPolicy(policies=[OnStartupTriggeringPolicy, SizeBasedTriggeringPolicy(size=10485760), TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=false)]), strategy=DefaultRolloverStrategy(min=1, max=5, useMax=true), advertiseURI=null, layout=%d{yyyy-MM-dd HH:mm:ss} %m%n, filePermissions=null, fileOwner=null]]
	at org.apache.logging.log4j.core.appender.AbstractManager.getManager(AbstractManager.java:148)
	at org.apache.logging.log4j.core.appender.OutputStreamManager.getManager(OutputStreamManager.java:112)
	at org.apache.logging.log4j.core.appender.rolling.RollingFileManager.getFileManager(RollingFileManager.java:307)
	at org.apache.logging.log4j.core.appender.RollingFileAppender$Builder.build(RollingFileAppender.java:144)
	at org.apache.logging.log4j.core.appender.RollingFileAppender$Builder.build(RollingFileAppender.java:65)
	at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:124)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:1180)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1099)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1091)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:695)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:270)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:319)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:673)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:762)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:784)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:300)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:46)
	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:138)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:136)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:58)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:46)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:32)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:447)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:472)
	at ai.djl.serving.ModelServer.<clinit>(ModelServer.java:69)

2025-09-23T05:12:59.978824621Z main ERROR Unable to invoke factory method in class org.apache.logging.log4j.core.appender.RollingFileAppender for element RollingFile: java.lang.IllegalStateException: No factory method found for class org.apache.logging.log4j.core.appender.RollingFileAppender java.lang.IllegalStateException: No factory method found for class org.apache.logging.log4j.core.appender.RollingFileAppender
	at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.findFactoryMethod(PluginBuilder.java:268)
	at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:140)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:1180)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1099)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1091)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:695)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:270)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:319)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:673)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:762)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:784)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:300)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:46)
	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:138)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:136)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:58)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:46)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:32)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:447)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:472)
	at ai.djl.serving.ModelServer.<clinit>(ModelServer.java:69)

2025-09-23T05:12:59.986780682Z main ERROR Null object returned for RollingFile in Appenders.
2025-09-23T05:12:59.986886230Z main ERROR Null object returned for RollingFile in Appenders.
2025-09-23T05:12:59.986970546Z main ERROR Null object returned for RollingFile in Appenders.
2025-09-23T05:12:59.987051964Z main ERROR Null object returned for RollingFile in Appenders.
2025-09-23T05:12:59.993100044Z main ERROR Unable to locate appender "application" for logger config "root"
2025-09-23T05:12:59.993217154Z main ERROR Unable to locate appender "access" for logger config "ACCESS_LOG"
2025-09-23T05:12:59.993302927Z main ERROR Unable to locate appender "server_metric" for logger config "server_metric"
2025-09-23T05:12:59.993373658Z main ERROR Unable to locate appender "application" for logger config "ai.djl"
2025-09-23T05:12:59.993434908Z main ERROR Unable to locate appender "model_metric" for logger config "model_metric"
[36mDEBUG[m [92mEngine[m Registering EngineProvider: TensorFlow
[36mDEBUG[m [92mEngine[m Registering EngineProvider: Rust
[36mDEBUG[m [92mEngine[m Registering EngineProvider: RPC
[36mDEBUG[m [92mEngine[m Registering EngineProvider: Python
[36mDEBUG[m [92mEngine[m Registering EngineProvider: MPI
[36mDEBUG[m [92mEngine[m Registering EngineProvider: PyTorch
[36mDEBUG[m [92mEngine[m Found default engine: PyTorch
[32mINFO [m [92mEc2Utils[m DJL will collect telemetry to help us better understand our users' needs, diagnose issues, and deliver additional features. If you would like to learn more or opt-out please go to: https://docs.djl.ai/master/docs/telemetry.html for more information.
[36mDEBUG[m [92mEngine[m Registering EngineProvider: OnnxRuntime
[32mINFO [m [92mModelServer[m Starting model server ...
[32mINFO [m [92mModelServer[m Starting djl-serving: 0.34.0-SNAPSHOT ...
[32mINFO [m [92mModelServer[m 
Model server home: /opt/djl
Current directory: /opt/djl
Temp directory: /tmp
Command line: -Dlog4j.configurationFile=/usr/local/djl-serving-0.34.0-SNAPSHOT/conf/log4j2.xml -Xmx1g -Xms1g -XX:+ExitOnOutOfMemoryError -Dai.djl.util.cuda.fork=true -Dai.djl.logging.level=debug
Number of CPUs: 96
CUDA version: 128 / 80
Number of GPUs: 8
Max heap size: 1024
Config file: /opt/djl/conf/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8080
Default job_queue_size: 1000
Default batch_size: 1
Default max_batch_delay: 100
Default max_idle_time: 60
Model Store: /opt/ml/model
Initial Models: test=file:/opt/ml/model/test/
Netty threads: 0
Maximum Request Size: 67108864
Environment variables:
    HF_HUB_ENABLE_HF_TRANSFER: 1
    HF_HOME: /tmp/.cache/huggingface
    OMP_NUM_THREADS: 1
    SERVING_OPTS: -Dai.djl.logging.level=debug
    SERVING_FEATURES: vllm
    DJL_CACHE_DIR: /tmp/.djl.ai
[32mINFO [m [92mFolderScanPluginManager[m scanning for plugins...
[32mINFO [m [92mFolderScanPluginManager[m scanning in plug-in folder :/opt/djl/plugins
[32mINFO [m [92mFolderScanPluginManager[m scanning in plug-in folder :/usr/local/djl-serving-0.34.0-SNAPSHOT/plugins
[32mINFO [m [92mPropertyFilePluginMetaDataReader[m Plugin found: cache-engines/jar:file:/usr/local/djl-serving-0.34.0-SNAPSHOT/plugins/cache-0.34.0-SNAPSHOT.jar!/META-INF/plugin.definition
[32mINFO [m [92mPropertyFilePluginMetaDataReader[m Plugin found: secure-mode/jar:file:/usr/local/djl-serving-0.34.0-SNAPSHOT/plugins/secure-mode-0.34.0-SNAPSHOT.jar!/META-INF/plugin.definition
[32mINFO [m [92mPropertyFilePluginMetaDataReader[m Plugin found: console/jar:file:/usr/local/djl-serving-0.34.0-SNAPSHOT/plugins/management-console-0.34.0-SNAPSHOT.jar!/META-INF/plugin.definition
[32mINFO [m [92mPropertyFilePluginMetaDataReader[m Plugin found: plugin-management/jar:file:/usr/local/djl-serving-0.34.0-SNAPSHOT/plugins/plugin-management-plugin-0.34.0-SNAPSHOT.jar!/META-INF/plugin.definition
[32mINFO [m [92mPropertyFilePluginMetaDataReader[m Plugin found: static-file-plugin/jar:file:/usr/local/djl-serving-0.34.0-SNAPSHOT/plugins/static-file-plugin-0.34.0-SNAPSHOT.jar!/META-INF/plugin.definition
[32mINFO [m [92mPropertyFilePluginMetaDataReader[m Plugin found: kserve/jar:file:/usr/local/djl-serving-0.34.0-SNAPSHOT/plugins/kserve-0.34.0-SNAPSHOT.jar!/META-INF/plugin.definition
[32mINFO [m [92mFolderScanPluginManager[m Loading plugin: {console/jar:file:/usr/local/djl-serving-0.34.0-SNAPSHOT/plugins/management-console-0.34.0-SNAPSHOT.jar!/META-INF/plugin.definition}
[32mINFO [m [92mPluginMetaData[m plugin console changed state to INITIALIZED
[32mINFO [m [92mFolderScanPluginManager[m Loading plugin: {static-file-plugin/jar:file:/usr/local/djl-serving-0.34.0-SNAPSHOT/plugins/static-file-plugin-0.34.0-SNAPSHOT.jar!/META-INF/plugin.definition}
[32mINFO [m [92mPluginMetaData[m plugin static-file-plugin changed state to INITIALIZED
[32mINFO [m [92mFolderScanPluginManager[m Loading plugin: {plugin-management/jar:file:/usr/local/djl-serving-0.34.0-SNAPSHOT/plugins/plugin-management-plugin-0.34.0-SNAPSHOT.jar!/META-INF/plugin.definition}
[32mINFO [m [92mPluginMetaData[m plugin plugin-management changed state to INITIALIZED
[32mINFO [m [92mFolderScanPluginManager[m Loading plugin: {cache-engines/jar:file:/usr/local/djl-serving-0.34.0-SNAPSHOT/plugins/cache-0.34.0-SNAPSHOT.jar!/META-INF/plugin.definition}
[32mINFO [m [92mPluginMetaData[m plugin cache-engines changed state to INITIALIZED
[32mINFO [m [92mFolderScanPluginManager[m Loading plugin: {secure-mode/jar:file:/usr/local/djl-serving-0.34.0-SNAPSHOT/plugins/secure-mode-0.34.0-SNAPSHOT.jar!/META-INF/plugin.definition}
[32mINFO [m [92mPluginMetaData[m plugin secure-mode changed state to INITIALIZED
[32mINFO [m [92mFolderScanPluginManager[m Loading plugin: {kserve/jar:file:/usr/local/djl-serving-0.34.0-SNAPSHOT/plugins/kserve-0.34.0-SNAPSHOT.jar!/META-INF/plugin.definition}
[32mINFO [m [92mPluginMetaData[m plugin kserve changed state to INITIALIZED
[32mINFO [m [92mPluginMetaData[m plugin console changed state to ACTIVE reason: plugin ready
[32mINFO [m [92mPluginMetaData[m plugin static-file-plugin changed state to ACTIVE reason: plugin ready
[32mINFO [m [92mPluginMetaData[m plugin plugin-management changed state to ACTIVE reason: plugin ready
[32mINFO [m [92mPluginMetaData[m plugin cache-engines changed state to ACTIVE reason: plugin ready
[32mINFO [m [92mPluginMetaData[m plugin secure-mode changed state to ACTIVE reason: plugin ready
[32mINFO [m [92mPluginMetaData[m plugin kserve changed state to ACTIVE reason: plugin ready
[32mINFO [m [92mFolderScanPluginManager[m 6 plug-ins found and loaded.
[32mINFO [m [92mModelServer[m Initializing model: test=file:/opt/ml/model/test/
[36mDEBUG[m [92mSimpleRepository[m Skip prepare for local repository.
[32mINFO [m [92mModelInfo[m M-0001: S3 url found, start downloading from s3://djl-llm/gpt-neox-20b-custom
[36mDEBUG[m [92mLmiUtils[m cp s3://djl-llm/gpt-neox-20b-custom/integration_script.py /tmp/.djl.ai/download/tmp16987650241517606706/integration_script.py
cp s3://djl-llm/gpt-neox-20b-custom/config.json /tmp/.djl.ai/download/tmp16987650241517606706/config.json
cp s3://djl-llm/gpt-neox-20b-custom/tokenizer_config.json /tmp/.djl.ai/download/tmp16987650241517606706/tokenizer_config.json
cp s3://djl-llm/gpt-neox-20b-custom/special_tokens_map.json /tmp/.djl.ai/download/tmp16987650241517606706/special_tokens_map.json
cp s3://djl-llm/gpt-neox-20b-custom/model.py /tmp/.djl.ai/download/tmp16987650241517606706/model.py
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model.bin.index.json /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model.bin.index.json
cp s3://djl-llm/gpt-neox-20b-custom/merges.txt /tmp/.djl.ai/download/tmp16987650241517606706/merges.txt
cp s3://djl-llm/gpt-neox-20b-custom/tokenizer.json /tmp/.djl.ai/download/tmp16987650241517606706/tokenizer.json
cp s3://djl-llm/gpt-neox-20b-custom/vocab.json /tmp/.djl.ai/download/tmp16987650241517606706/vocab.json
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00045-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00045-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00036-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00036-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00040-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00040-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00024-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00024-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00023-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00023-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00014-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00014-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00004-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00004-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00019-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00019-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00017-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00017-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00006-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00006-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00012-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00012-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00002-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00002-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00011-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00011-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00005-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00005-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00021-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00021-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00033-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00033-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00001-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00001-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00015-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00015-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00022-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00022-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00038-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00038-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00020-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00020-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00031-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00031-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00044-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00044-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00046-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00046-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00018-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00018-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00029-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00029-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00003-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00003-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00026-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00026-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00035-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00035-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00027-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00027-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00030-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00030-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00009-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00009-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00028-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00028-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00039-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00039-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00016-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00016-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00034-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00034-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00013-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00013-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00032-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00032-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00008-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00008-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00041-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00041-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00007-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00007-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00025-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00025-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00042-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00042-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00037-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00037-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00043-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00043-of-00046.bin
cp s3://djl-llm/gpt-neox-20b-custom/pytorch_model-00010-of-00046.bin /tmp/.djl.ai/download/tmp16987650241517606706/pytorch_model-00010-of-00046.bin

[32mINFO [m [92mModelInfo[m M-0001: Download completed! Files saved to /tmp/.djl.ai/download/830936cbbc7c8d2253b4ebe798f7cfc0262b8919
[36mDEBUG[m [92mLmiUtils[m Found config file: config.json in downloadDir /tmp/.djl.ai/download/830936cbbc7c8d2253b4ebe798f7cfc0262b8919
[32mINFO [m [92mLmiUtils[m Detected mpi_mode: null, rolling_batch: disable, tensor_parallel_degree: 4, for modelType: gpt_neox
[32mINFO [m [92mModelInfo[m M-0001: Apply per model settings:
    job_queue_size: 1000
    max_dynamic_batch_size: 1
    max_batch_delay: 100
    max_idle_time: 60
    load_on_devices: *
    engine: Python
    mpi_mode: null
    option.entryPoint: djl_python.lmi_vllm.vllm_async_service
    option.task: text-generation
    option.tensor_parallel_degree: 4
    option.max_rolling_batch_size: 32
    option.pipeline_parallel_degree: 1
    option.async_mode: true
    option.model_id: s3://djl-llm/gpt-neox-20b-custom
    option.rolling_batch: disable
[32mINFO [m [92mPlatform[m Found matching platform from: jar:file:/usr/local/djl-serving-0.34.0-SNAPSHOT/lib/python-0.34.0-SNAPSHOT.jar!/native/lib/python.properties
[36mDEBUG[m [92mPyEnv[m Using cache dir: /tmp/.djl.ai/python
[32mINFO [m [92mPyEnv[m Extracting /djl_python_engine.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/__init__.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/arg_parser.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/async_utils.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/aws/__init__.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/aws/cloud_watch.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/chat_completions/__init__.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/chat_completions/chat_properties.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/chat_completions/chat_utils.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/chat_completions/vllm_chat_utils.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/encode_decode.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/huggingface.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/input_parser.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/inputs.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/lmi_trtllm/request_response_utils.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/lmi_trtllm/trtllm_async_service.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/lmi_vllm/request_response_utils.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/lmi_vllm/vllm_async_service.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/multimodal/__init__.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/multimodal/utils.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/neuron_utils/__init__.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/neuron_utils/model_loader.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/neuron_utils/neuron_smart_default_utils.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/neuron_utils/utils.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/np_util.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/output_formatter.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/outputs.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/pair_list.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/properties_manager/__init__.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/properties_manager/hf_properties.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/properties_manager/lmi_dist_rb_properties.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/properties_manager/properties.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/properties_manager/sd_inf2_properties.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/properties_manager/tnx_properties.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/properties_manager/trt_properties.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/properties_manager/vllm_rb_properties.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/python_async_engine.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/python_sync_engine.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/request.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/request_io.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/rolling_batch/__init__.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/rolling_batch/lmi_dist_rolling_batch.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/rolling_batch/neuron_rolling_batch.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/rolling_batch/rolling_batch.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/rolling_batch/rolling_batch_vllm_utils.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/rolling_batch/trtllm_rolling_batch.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/rolling_batch/vllm_rolling_batch.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/sagemaker.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/service_loader.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/session_manager.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/sm_log_filter.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/stable_diffusion_inf2.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/streaming_utils.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/telemetry.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/tensorrt_llm.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/test_model.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/three_p/__init__.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/three_p/three_p_utils.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/transformers_neuronx.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/transformers_neuronx_scheduler/__init__.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/transformers_neuronx_scheduler/optimum_modeling.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/transformers_neuronx_scheduler/optimum_neuron_scheduler.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/transformers_neuronx_scheduler/optimum_token_selector.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/transformers_neuronx_scheduler/slot.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/transformers_neuronx_scheduler/speculation.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/transformers_neuronx_scheduler/utils.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/ts_service_loader.py to cache ...
[32mINFO [m [92mPyEnv[m Extracting /djl_python/utils.py to cache ...
[32mINFO [m [92mModelManager[m Loading model on Python:[0, 4]
[32mINFO [m [92mWorkerPool[m loading model test (M-0001, PENDING) on gpu(0) ...
[32mINFO [m [92mModelInfo[m M-0001: Available CPU memory: 1121451 MB, required: 0 MB, reserved: 500 MB
[32mINFO [m [92mModelInfo[m M-0001: Available GPU memory: 80730 MB, required: 0 MB, reserved: 500 MB
[36mDEBUG[m [92mDefaultModelZoo[m Scanning models in repo: class ai.djl.repository.SimpleRepository, file:/opt/ml/model/test/
[32mINFO [m [92mModelInfo[m Loading model test M-0001 on gpu(0)
[36mDEBUG[m [92mModelZoo[m Loading model with Criteria:
	Application: UNDEFINED
	Input: class ai.djl.modality.Input
	Output: class ai.djl.modality.Output
	Engine: Python
	ModelZoo: ai.djl.localmodelzoo
	Arguments: {"engine":"Python"}
	Options: {"task":"text-generation","tensor_parallel_degree":"4","rolling_batch":"disable","entryPoint":"djl_python.lmi_vllm.vllm_async_service","model_id":"/tmp/.djl.ai/download/830936cbbc7c8d2253b4ebe798f7cfc0262b8919","max_rolling_batch_size":"32","async_mode":"true","pipeline_parallel_degree":"1"}

[36mDEBUG[m [92mModelZoo[m Searching model in specified model zoo: ai.djl.localmodelzoo
[36mDEBUG[m [92mModelZoo[m Checking ModelLoader: djl://ai.djl.localmodelzoo/test [
	ai.djl.localmodelzoo/test/test {}
]
[36mDEBUG[m [92mMRL[m Preparing artifact: file:/opt/ml/model/test/, ai.djl.localmodelzoo/test/test {}
[36mDEBUG[m [92mSimpleRepository[m Skip prepare for local repository.
[36mDEBUG[m [92mPyModel[m options in serving.properties for model: test
[36mDEBUG[m [92mPyModel[m model_id=/tmp/.djl.ai/download/830936cbbc7c8d2253b4ebe798f7cfc0262b8919
[36mDEBUG[m [92mPyModel[m max_rolling_batch_size=32
[36mDEBUG[m [92mPyModel[m async_mode=true
[36mDEBUG[m [92mPyModel[m task=text-generation
[36mDEBUG[m [92mPyModel[m tensor_parallel_degree=4
[36mDEBUG[m [92mPyModel[m rolling_batch=disable
[36mDEBUG[m [92mPyModel[m entryPoint=djl_python.lmi_vllm.vllm_async_service
[36mDEBUG[m [92mPyModel[m pipeline_parallel_degree=1
[32mINFO [m [92mWorkerPool[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1
[32mINFO [m [92mPyProcess[m Creating new python process in async mode
[32mINFO [m [92mPyProcess[m Start process: 19000 - retry: 0
[32mINFO [m [92mConnection[m Set CUDA_VISIBLE_DEVICES=0,1,2,3
[36mDEBUG[m [92mConnection[m cmd: [python3, /tmp/.djl.ai/python/0.34.0-SNAPSHOT/djl_python_engine.py, --sock-type, unix, --sock-name, /tmp/djl_sock.19000, --model-dir, /opt/ml/model/test, --entry-point, djl_python.lmi_vllm.vllm_async_service, --device-id, 0, --cluster-size, 1, --recommended-entry-point, , --log-level, debug, --async-mode]
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::262 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--model-dir', '/opt/ml/model/test', '--entry-point', 'djl_python.lmi_vllm.vllm_async_service', '--device-id', '0', '--cluster-size', '1', '--recommended-entry-point', '', '--log-level', 'debug', '--async-mode']
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:33 [__init__.py:216] Automatically detected platform cuda.
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::Found decorated function: custom_output_formatter for decorator attribute: is_output_formatter
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::Found decorated function: custom_output_formatter for decorator attribute: is_output_formatter
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::Found decorated function: custom_input_formatter for decorator attribute: is_input_formatter
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::Found decorated function: custom_input_formatter for decorator attribute: is_input_formatter
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::Python engine started.
[36mDEBUG[m [92mConnection[m Connecting to socket: /tmp/djl_sock.19000
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::starting receive requests thread
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::starting send responses thread
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id None, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::Found decorated function: custom_input_formatter for decorator attribute: is_input_formatter
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::Found decorated function: custom_input_formatter for decorator attribute: is_input_formatter
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::Found decorated function: custom_output_formatter for decorator attribute: is_output_formatter
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::Found decorated function: custom_output_formatter for decorator attribute: is_output_formatter
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::Loaded formatters - input: <function custom_input_formatter at 0x757b65618680>, output: <function custom_output_formatter at 0x757b6565d120>
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::Construction vLLM engine args from the following DJL configs: {'model': '/tmp/.djl.ai/download/830936cbbc7c8d2253b4ebe798f7cfc0262b8919', 'tensor_parallel_size': 4, 'pipeline_parallel_size': 1, 'max_num_seqs': 32, 'dtype': 'auto', 'revision': None, 'max_loras': 4, 'enable_lora': False, 'trust_remote_code': False, 'cpu_offload_gb': 0, 'quantization': None}
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:42 [__init__.py:742] Resolved architecture: GPTNeoXForCausalLM
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:42 [__init__.py:1815] Using max model len 2048
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:42 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:47 [__init__.py:216] Automatically detected platform cuda.
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(EngineCore_DP0 pid=300)[0;0m INFO 09-23 05:13:48 [core.py:654] Waiting for init message from front-end.
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(EngineCore_DP0 pid=300)[0;0m INFO 09-23 05:13:48 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='/tmp/.djl.ai/download/830936cbbc7c8d2253b4ebe798f7cfc0262b8919', speculative_config=None, tokenizer='/tmp/.djl.ai/download/830936cbbc7c8d2253b4ebe798f7cfc0262b8919', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/tmp/.djl.ai/download/830936cbbc7c8d2253b4ebe798f7cfc0262b8919, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":64,"local_cache_dir":null}
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(EngineCore_DP0 pid=300)[0;0m INFO 09-23 05:13:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_560222dd'), local_subscribe_addr='ipc:///tmp/2dadcbaf-14fb-4041-86e7-c24eced10964', remote_subscribe_addr=None, remote_addr_ipv6=False)
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:53 [__init__.py:216] Automatically detected platform cuda.
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:53 [__init__.py:216] Automatically detected platform cuda.
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:53 [__init__.py:216] Automatically detected platform cuda.
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:53 [__init__.py:216] Automatically detected platform cuda.
[33mWARN [m [92mPyProcess[m W-262-test-stderr: W0923 05:13:55.133000 309 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: W0923 05:13:55.133000 309 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[33mWARN [m [92mPyProcess[m W-262-test-stderr: W0923 05:13:55.179000 311 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: W0923 05:13:55.179000 311 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[33mWARN [m [92mPyProcess[m W-262-test-stderr: 2025-09-23 05:13:55,358 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend
[33mWARN [m [92mPyProcess[m W-262-test-stderr: W0923 05:13:55.368000 310 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: W0923 05:13:55.368000 310 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[33mWARN [m [92mPyProcess[m W-262-test-stderr: W0923 05:13:55.371000 312 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: W0923 05:13:55.371000 312 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[33mWARN [m [92mPyProcess[m W-262-test-stderr: 2025-09-23 05:13:55,554 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend
[33mWARN [m [92mPyProcess[m W-262-test-stderr: 2025-09-23 05:13:55,779 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend
[33mWARN [m [92mPyProcess[m W-262-test-stderr: 2025-09-23 05:13:55,786 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:56 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_da1fe70f'), local_subscribe_addr='ipc:///tmp/d64af30c-f6db-4d4a-a8a3-5016a8671078', remote_subscribe_addr=None, remote_addr_ipv6=False)
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:56 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f4597963'), local_subscribe_addr='ipc:///tmp/db07382c-b20d-48bd-b0d6-fb06d5bed4fc', remote_subscribe_addr=None, remote_addr_ipv6=False)
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:56 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f81849c1'), local_subscribe_addr='ipc:///tmp/49291263-7188-4b99-b30e-d799fa38c587', remote_subscribe_addr=None, remote_addr_ipv6=False)
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:57 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ce85537d'), local_subscribe_addr='ipc:///tmp/3ba27274-7f98-4206-bb54-55f6da77e164', remote_subscribe_addr=None, remote_addr_ipv6=False)
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [W923 05:13:57.683780421 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [W923 05:13:57.826052421 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [W923 05:13:57.829337120 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [W923 05:13:57.833413645 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:57 [__init__.py:1433] Found nccl from library libnccl.so.2
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:57 [__init__.py:1433] Found nccl from library libnccl.so.2
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:57 [pynccl.py:70] vLLM is using nccl==2.27.3
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:57 [pynccl.py:70] vLLM is using nccl==2.27.3
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:57 [__init__.py:1433] Found nccl from library libnccl.so.2
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:57 [pynccl.py:70] vLLM is using nccl==2.27.3
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:57 [__init__.py:1433] Found nccl from library libnccl.so.2
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:57 [pynccl.py:70] vLLM is using nccl==2.27.3
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:58 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:58 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:58 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:58 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_99050bae'), local_subscribe_addr='ipc:///tmp/851656fe-b87f-4d5a-93ac-f10089bfe33d', remote_subscribe_addr=None, remote_addr_ipv6=False)
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:58 [parallel_state.py:1165] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:58 [parallel_state.py:1165] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:58 [parallel_state.py:1165] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:58 [parallel_state.py:1165] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:58 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:58 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:58 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:13:58 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP1 pid=310)[0;0m INFO 09-23 05:13:58 [gpu_model_runner.py:2338] Starting to load model /tmp/.djl.ai/download/830936cbbc7c8d2253b4ebe798f7cfc0262b8919...
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP3 pid=312)[0;0m INFO 09-23 05:13:58 [gpu_model_runner.py:2338] Starting to load model /tmp/.djl.ai/download/830936cbbc7c8d2253b4ebe798f7cfc0262b8919...
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP0 pid=309)[0;0m INFO 09-23 05:13:58 [gpu_model_runner.py:2338] Starting to load model /tmp/.djl.ai/download/830936cbbc7c8d2253b4ebe798f7cfc0262b8919...
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP2 pid=311)[0;0m INFO 09-23 05:13:58 [gpu_model_runner.py:2338] Starting to load model /tmp/.djl.ai/download/830936cbbc7c8d2253b4ebe798f7cfc0262b8919...
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP3 pid=312)[0;0m INFO 09-23 05:13:58 [gpu_model_runner.py:2370] Loading model from scratch...
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP1 pid=310)[0;0m INFO 09-23 05:13:58 [gpu_model_runner.py:2370] Loading model from scratch...
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP2 pid=311)[0;0m INFO 09-23 05:13:58 [gpu_model_runner.py:2370] Loading model from scratch...
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP0 pid=309)[0;0m INFO 09-23 05:13:58 [gpu_model_runner.py:2370] Loading model from scratch...
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP3 pid=312)[0;0m INFO 09-23 05:13:58 [cuda.py:362] Using Flash Attention backend on V1 engine.
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP1 pid=310)[0;0m INFO 09-23 05:13:58 [cuda.py:362] Using Flash Attention backend on V1 engine.
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP2 pid=311)[0;0m INFO 09-23 05:13:58 [cuda.py:362] Using Flash Attention backend on V1 engine.
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP0 pid=309)[0;0m INFO 09-23 05:13:58 [cuda.py:362] Using Flash Attention backend on V1 engine.
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:   0% Completed | 0/46 [00:00<?, ?it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:   2% Completed | 1/46 [00:00<00:36,  1.23it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:   4% Completed | 2/46 [00:01<00:35,  1.24it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:   7% Completed | 3/46 [00:02<00:34,  1.25it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:   9% Completed | 4/46 [00:03<00:33,  1.26it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  11% Completed | 5/46 [00:03<00:32,  1.26it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  13% Completed | 6/46 [00:04<00:31,  1.26it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  15% Completed | 7/46 [00:05<00:30,  1.26it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  17% Completed | 8/46 [00:06<00:30,  1.26it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  20% Completed | 9/46 [00:06<00:24,  1.49it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  22% Completed | 10/46 [00:07<00:25,  1.39it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  24% Completed | 11/46 [00:08<00:25,  1.35it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  26% Completed | 12/46 [00:09<00:25,  1.33it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  28% Completed | 13/46 [00:09<00:25,  1.31it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  30% Completed | 14/46 [00:10<00:24,  1.30it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  33% Completed | 15/46 [00:11<00:24,  1.29it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  35% Completed | 16/46 [00:12<00:23,  1.28it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  37% Completed | 17/46 [00:13<00:22,  1.28it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  39% Completed | 18/46 [00:13<00:22,  1.27it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  41% Completed | 19/46 [00:14<00:21,  1.27it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  43% Completed | 20/46 [00:15<00:20,  1.26it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  46% Completed | 21/46 [00:16<00:19,  1.26it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  48% Completed | 22/46 [00:17<00:19,  1.26it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  50% Completed | 23/46 [00:17<00:18,  1.25it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  52% Completed | 24/46 [00:18<00:17,  1.25it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  54% Completed | 25/46 [00:19<00:16,  1.25it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  57% Completed | 26/46 [00:20<00:16,  1.25it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  59% Completed | 27/46 [00:21<00:15,  1.25it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  61% Completed | 28/46 [00:21<00:14,  1.25it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  63% Completed | 29/46 [00:22<00:13,  1.24it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  65% Completed | 30/46 [00:23<00:11,  1.43it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  67% Completed | 31/46 [00:23<00:10,  1.38it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  70% Completed | 32/46 [00:24<00:10,  1.34it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  72% Completed | 33/46 [00:25<00:09,  1.31it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  74% Completed | 34/46 [00:26<00:09,  1.29it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  76% Completed | 35/46 [00:27<00:08,  1.27it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  78% Completed | 36/46 [00:27<00:07,  1.27it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  80% Completed | 37/46 [00:28<00:07,  1.26it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  83% Completed | 38/46 [00:29<00:06,  1.26it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  85% Completed | 39/46 [00:30<00:05,  1.25it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  87% Completed | 40/46 [00:31<00:04,  1.25it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  89% Completed | 41/46 [00:31<00:04,  1.25it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  91% Completed | 42/46 [00:32<00:03,  1.25it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  93% Completed | 43/46 [00:33<00:02,  1.26it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  96% Completed | 44/46 [00:34<00:01,  1.26it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards:  98% Completed | 45/46 [00:35<00:00,  1.26it/s]
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP1 pid=310)[0;0m INFO 09-23 05:14:34 [default_loader.py:268] Loading weights took 35.98 seconds
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards: 100% Completed | 46/46 [00:35<00:00,  1.26it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Loading pt checkpoint shards: 100% Completed | 46/46 [00:35<00:00,  1.28it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP0 pid=309)[0;0m INFO 09-23 05:14:34 [default_loader.py:268] Loading weights took 35.95 seconds
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP3 pid=312)[0;0m INFO 09-23 05:14:34 [default_loader.py:268] Loading weights took 36.02 seconds
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP0 pid=309)[0;0m INFO 09-23 05:14:35 [gpu_model_runner.py:2392] Model loading took 9.5745 GiB and 36.197491 seconds
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP3 pid=312)[0;0m INFO 09-23 05:14:35 [gpu_model_runner.py:2392] Model loading took 9.5745 GiB and 36.258437 seconds
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP1 pid=310)[0;0m INFO 09-23 05:14:35 [gpu_model_runner.py:2392] Model loading took 9.5745 GiB and 36.217060 seconds
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP2 pid=311)[0;0m INFO 09-23 05:14:35 [default_loader.py:268] Loading weights took 36.91 seconds
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP2 pid=311)[0;0m INFO 09-23 05:14:36 [gpu_model_runner.py:2392] Model loading took 9.5745 GiB and 37.164266 seconds
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP3 pid=312)[0;0m INFO 09-23 05:14:43 [backends.py:539] Using cache directory: /home/djl/.cache/vllm/torch_compile_cache/83c76f34cf/rank_3_0/backbone for vLLM's torch.compile
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP3 pid=312)[0;0m INFO 09-23 05:14:43 [backends.py:550] Dynamo bytecode transform time: 7.23 s
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP2 pid=311)[0;0m INFO 09-23 05:14:43 [backends.py:539] Using cache directory: /home/djl/.cache/vllm/torch_compile_cache/83c76f34cf/rank_2_0/backbone for vLLM's torch.compile
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP2 pid=311)[0;0m INFO 09-23 05:14:43 [backends.py:550] Dynamo bytecode transform time: 7.32 s
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP1 pid=310)[0;0m INFO 09-23 05:14:43 [backends.py:539] Using cache directory: /home/djl/.cache/vllm/torch_compile_cache/83c76f34cf/rank_1_0/backbone for vLLM's torch.compile
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP1 pid=310)[0;0m INFO 09-23 05:14:43 [backends.py:550] Dynamo bytecode transform time: 7.35 s
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP0 pid=309)[0;0m INFO 09-23 05:14:43 [backends.py:539] Using cache directory: /home/djl/.cache/vllm/torch_compile_cache/83c76f34cf/rank_0_0/backbone for vLLM's torch.compile
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP0 pid=309)[0;0m INFO 09-23 05:14:43 [backends.py:550] Dynamo bytecode transform time: 7.42 s
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP3 pid=312)[0;0m INFO 09-23 05:14:47 [backends.py:194] Cache the graph for dynamic shape for later use
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP0 pid=309)[0;0m INFO 09-23 05:14:47 [backends.py:194] Cache the graph for dynamic shape for later use
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP2 pid=311)[0;0m INFO 09-23 05:14:47 [backends.py:194] Cache the graph for dynamic shape for later use
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP1 pid=310)[0;0m INFO 09-23 05:14:47 [backends.py:194] Cache the graph for dynamic shape for later use
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP3 pid=312)[0;0m INFO 09-23 05:15:16 [backends.py:215] Compiling a graph for dynamic shape takes 31.57 s
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP2 pid=311)[0;0m INFO 09-23 05:15:16 [backends.py:215] Compiling a graph for dynamic shape takes 31.61 s
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP0 pid=309)[0;0m INFO 09-23 05:15:17 [backends.py:215] Compiling a graph for dynamic shape takes 32.18 s
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP1 pid=310)[0;0m INFO 09-23 05:15:17 [backends.py:215] Compiling a graph for dynamic shape takes 32.29 s
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP2 pid=311)[0;0m INFO 09-23 05:15:22 [monitor.py:34] torch.compile takes 38.93 s in total
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP0 pid=309)[0;0m INFO 09-23 05:15:22 [monitor.py:34] torch.compile takes 39.60 s in total
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP1 pid=310)[0;0m INFO 09-23 05:15:22 [monitor.py:34] torch.compile takes 39.64 s in total
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP3 pid=312)[0;0m INFO 09-23 05:15:22 [monitor.py:34] torch.compile takes 38.80 s in total
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 2025-09-23 05:15:22,557 - INFO - flashinfer.jit: Loading JIT ops: sampling
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP3 pid=312)[0;0m 2025-09-23 05:15:22,557 - INFO - flashinfer.jit: Loading JIT ops: sampling
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP2 pid=311)[0;0m 2025-09-23 05:15:22,557 - INFO - flashinfer.jit: Loading JIT ops: sampling
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m [rank0]:W0923 05:15:22.557000 309 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m [rank0]:W0923 05:15:22.557000 309 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP3 pid=312)[0;0m [rank3]:W0923 05:15:22.557000 312 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP3 pid=312)[0;0m [rank3]:W0923 05:15:22.557000 312 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP2 pid=311)[0;0m [rank2]:W0923 05:15:22.557000 311 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP2 pid=311)[0;0m [rank2]:W0923 05:15:22.557000 311 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP1 pid=310)[0;0m 2025-09-23 05:15:22,559 - INFO - flashinfer.jit: Loading JIT ops: sampling
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP1 pid=310)[0;0m [rank1]:W0923 05:15:22.560000 310 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP1 pid=310)[0;0m [rank1]:W0923 05:15:22.560000 310 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m [rank0]:W0923 05:15:22.563000 309 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m [rank0]:W0923 05:15:22.563000 309 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 2025-09-23 05:16:05,338 - INFO - flashinfer.jit: Finished loading JIT ops: sampling
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP3 pid=312)[0;0m [rank3]:W0923 05:16:05.386000 312 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP3 pid=312)[0;0m [rank3]:W0923 05:16:05.386000 312 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP3 pid=312)[0;0m 2025-09-23 05:16:05,399 - INFO - flashinfer.jit: Finished loading JIT ops: sampling
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP1 pid=310)[0;0m [rank1]:W0923 05:16:05.440000 310 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP1 pid=310)[0;0m [rank1]:W0923 05:16:05.440000 310 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP1 pid=310)[0;0m 2025-09-23 05:16:05,456 - INFO - flashinfer.jit: Finished loading JIT ops: sampling
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP2 pid=311)[0;0m [rank2]:W0923 05:16:05.489000 311 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP2 pid=311)[0;0m [rank2]:W0923 05:16:05.489000 311 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP2 pid=311)[0;0m 2025-09-23 05:16:05,503 - INFO - flashinfer.jit: Finished loading JIT ops: sampling
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP0 pid=309)[0;0m INFO 09-23 05:16:06 [gpu_worker.py:298] Available KV cache memory: 60.45 GiB
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP3 pid=312)[0;0m INFO 09-23 05:16:06 [gpu_worker.py:298] Available KV cache memory: 60.45 GiB
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP1 pid=310)[0;0m INFO 09-23 05:16:06 [gpu_worker.py:298] Available KV cache memory: 60.45 GiB
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP2 pid=311)[0;0m INFO 09-23 05:16:06 [gpu_worker.py:298] Available KV cache memory: 60.45 GiB
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(EngineCore_DP0 pid=300)[0;0m INFO 09-23 05:16:06 [kv_cache_utils.py:864] GPU KV cache size: 240,096 tokens
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(EngineCore_DP0 pid=300)[0;0m INFO 09-23 05:16:06 [kv_cache_utils.py:868] Maximum concurrency for 2,048 tokens per request: 117.23x
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(EngineCore_DP0 pid=300)[0;0m INFO 09-23 05:16:06 [kv_cache_utils.py:864] GPU KV cache size: 240,096 tokens
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(EngineCore_DP0 pid=300)[0;0m INFO 09-23 05:16:06 [kv_cache_utils.py:868] Maximum concurrency for 2,048 tokens per request: 117.23x
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(EngineCore_DP0 pid=300)[0;0m INFO 09-23 05:16:06 [kv_cache_utils.py:864] GPU KV cache size: 240,096 tokens
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(EngineCore_DP0 pid=300)[0;0m INFO 09-23 05:16:06 [kv_cache_utils.py:868] Maximum concurrency for 2,048 tokens per request: 117.23x
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(EngineCore_DP0 pid=300)[0;0m INFO 09-23 05:16:06 [kv_cache_utils.py:864] GPU KV cache size: 240,096 tokens
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(EngineCore_DP0 pid=300)[0;0m INFO 09-23 05:16:06 [kv_cache_utils.py:868] Maximum concurrency for 2,048 tokens per request: 117.23x
[33mWARN [m [92mPyProcess[m W-262-test-stderr: [1;36m(Worker_TP0 pid=309)[0;0m 
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|??        | 2/11 [00:00<00:00, 19.16it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|?????     | 5/11 [00:00<00:00, 20.34it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|????????  | 8/11 [00:00<00:00, 20.67it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|??????????| 11/11 [00:00<00:00, 19.72it/s]
[33mWARN [m [92mPyProcess[m W-262-test-stderr: Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|??????????| 11/11 [00:00<00:00, 19.91it/s]
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP0 pid=309)[0;0m INFO 09-23 05:16:07 [custom_all_reduce.py:203] Registering 979 cuda graph addresses
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP3 pid=312)[0;0m INFO 09-23 05:16:07 [custom_all_reduce.py:203] Registering 979 cuda graph addresses
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP2 pid=311)[0;0m INFO 09-23 05:16:08 [custom_all_reduce.py:203] Registering 979 cuda graph addresses
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP1 pid=310)[0;0m INFO 09-23 05:16:08 [custom_all_reduce.py:203] Registering 979 cuda graph addresses
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP0 pid=309)[0;0m INFO 09-23 05:16:08 [gpu_model_runner.py:3118] Graph capturing finished in 1 secs, took 0.15 GiB
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP0 pid=309)[0;0m INFO 09-23 05:16:08 [gpu_worker.py:391] Free memory on device (78.76/79.25 GiB) on startup. Desired GPU memory utilization is (0.9, 71.33 GiB). Actual usage is 9.57 GiB for weight, 0.21 GiB for peak activation, 1.1 GiB for non-torch memory, and 0.15 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=64590292992` to fit into requested memory, or `--kv-cache-memory=72576837632` to fully utilize gpu memory. Current kv cache memory in use is 64906962944 bytes.
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP3 pid=312)[0;0m INFO 09-23 05:16:08 [gpu_model_runner.py:3118] Graph capturing finished in 1 secs, took 0.15 GiB
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP3 pid=312)[0;0m INFO 09-23 05:16:08 [gpu_worker.py:391] Free memory on device (78.76/79.25 GiB) on startup. Desired GPU memory utilization is (0.9, 71.33 GiB). Actual usage is 9.57 GiB for weight, 0.21 GiB for peak activation, 1.1 GiB for non-torch memory, and 0.15 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=64590292992` to fit into requested memory, or `--kv-cache-memory=72576837632` to fully utilize gpu memory. Current kv cache memory in use is 64906962944 bytes.
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP2 pid=311)[0;0m INFO 09-23 05:16:08 [gpu_model_runner.py:3118] Graph capturing finished in 1 secs, took 0.15 GiB
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP2 pid=311)[0;0m INFO 09-23 05:16:08 [gpu_worker.py:391] Free memory on device (78.76/79.25 GiB) on startup. Desired GPU memory utilization is (0.9, 71.33 GiB). Actual usage is 9.57 GiB for weight, 0.21 GiB for peak activation, 1.1 GiB for non-torch memory, and 0.15 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=64590292992` to fit into requested memory, or `--kv-cache-memory=72576837632` to fully utilize gpu memory. Current kv cache memory in use is 64906962944 bytes.
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP1 pid=310)[0;0m INFO 09-23 05:16:08 [gpu_model_runner.py:3118] Graph capturing finished in 1 secs, took 0.15 GiB
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(Worker_TP1 pid=310)[0;0m INFO 09-23 05:16:08 [gpu_worker.py:391] Free memory on device (78.76/79.25 GiB) on startup. Desired GPU memory utilization is (0.9, 71.33 GiB). Actual usage is 9.57 GiB for weight, 0.21 GiB for peak activation, 1.1 GiB for non-torch memory, and 0.15 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=64590292992` to fit into requested memory, or `--kv-cache-memory=72576837632` to fully utilize gpu memory. Current kv cache memory in use is 64906962944 bytes.
[32mINFO [m [92mPyProcess[m W-262-test-stdout: [1;36m(EngineCore_DP0 pid=300)[0;0m INFO 09-23 05:16:08 [core.py:218] init engine (profile, create kv cache, warmup model) took 92.23 seconds
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:16:08 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 15006
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO 09-23 05:16:08 [async_llm.py:180] Torch profiler disabled. AsyncLLM CPU traces will not be collected.
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::vllm service initialized
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::empty response received from service.invoke_handler_async()
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m process is not ready
[32mINFO [m [92mPyProcess[m Model [test] initialized.
[32mINFO [m [92mWorkerThread[m Starting worker thread WT-0001 for model test (M-0001, READY) on device gpu(0)
[36mDEBUG[m [92mWorkerPool[m worker pool for model test (M-0001, READY):
 WT-0001-fixedPool

[32mINFO [m [92mWorkerPool[m loading model test (M-0001, READY) on gpu(4) ...
[32mINFO [m [92mModelInfo[m M-0001: Available CPU memory: 1108249 MB, required: 0 MB, reserved: 500 MB
[32mINFO [m [92mModelInfo[m M-0001: Available GPU memory: 80730 MB, required: 0 MB, reserved: 500 MB
[36mDEBUG[m [92mDefaultModelZoo[m Scanning models in repo: class ai.djl.repository.SimpleRepository, file:/opt/ml/model/test/
[32mINFO [m [92mModelInfo[m Loading model test M-0001 on gpu(4)
[36mDEBUG[m [92mModelZoo[m Loading model with Criteria:
	Application: UNDEFINED
	Input: class ai.djl.modality.Input
	Output: class ai.djl.modality.Output
	Engine: Python
	ModelZoo: ai.djl.localmodelzoo
	Arguments: {"engine":"Python"}
	Options: {"task":"text-generation","tensor_parallel_degree":"4","rolling_batch":"disable","entryPoint":"djl_python.lmi_vllm.vllm_async_service","model_id":"/tmp/.djl.ai/download/830936cbbc7c8d2253b4ebe798f7cfc0262b8919","max_rolling_batch_size":"32","async_mode":"true","pipeline_parallel_degree":"1"}

[36mDEBUG[m [92mModelZoo[m Searching model in specified model zoo: ai.djl.localmodelzoo
[36mDEBUG[m [92mModelZoo[m Checking ModelLoader: djl://ai.djl.localmodelzoo/test [
	ai.djl.localmodelzoo/test/test {}
]
[36mDEBUG[m [92mMRL[m Preparing artifact: file:/opt/ml/model/test/, ai.djl.localmodelzoo/test/test {}
[36mDEBUG[m [92mSimpleRepository[m Skip prepare for local repository.
[36mDEBUG[m [92mPyModel[m options in serving.properties for model: test
[36mDEBUG[m [92mPyModel[m model_id=/tmp/.djl.ai/download/830936cbbc7c8d2253b4ebe798f7cfc0262b8919
[36mDEBUG[m [92mPyModel[m max_rolling_batch_size=32
[36mDEBUG[m [92mPyModel[m async_mode=true
[36mDEBUG[m [92mPyModel[m task=text-generation
[36mDEBUG[m [92mPyModel[m tensor_parallel_degree=4
[36mDEBUG[m [92mPyModel[m rolling_batch=disable
[36mDEBUG[m [92mPyModel[m entryPoint=djl_python.lmi_vllm.vllm_async_service
[36mDEBUG[m [92mPyModel[m pipeline_parallel_degree=1
[32mINFO [m [92mWorkerPool[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1
[32mINFO [m [92mPyProcess[m Creating new python process in async mode
[32mINFO [m [92mPyProcess[m Start process: 19001 - retry: 0
[32mINFO [m [92mConnection[m Set CUDA_VISIBLE_DEVICES=4,5,6,7
[36mDEBUG[m [92mConnection[m cmd: [python3, /tmp/.djl.ai/python/0.34.0-SNAPSHOT/djl_python_engine.py, --sock-type, unix, --sock-name, /tmp/djl_sock.19001, --model-dir, /opt/ml/model/test, --entry-point, djl_python.lmi_vllm.vllm_async_service, --device-id, 0, --cluster-size, 1, --recommended-entry-point, , --log-level, debug, --async-mode]
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::2039 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19001', '--model-dir', '/opt/ml/model/test', '--entry-point', 'djl_python.lmi_vllm.vllm_async_service', '--device-id', '0', '--cluster-size', '1', '--recommended-entry-point', '', '--log-level', 'debug', '--async-mode']
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:15 [__init__.py:216] Automatically detected platform cuda.
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::Found decorated function: custom_output_formatter for decorator attribute: is_output_formatter
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::Found decorated function: custom_output_formatter for decorator attribute: is_output_formatter
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::Found decorated function: custom_input_formatter for decorator attribute: is_input_formatter
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::Found decorated function: custom_input_formatter for decorator attribute: is_input_formatter
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::Python engine started.
[36mDEBUG[m [92mConnection[m Connecting to socket: /tmp/djl_sock.19001
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::starting receive requests thread
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::starting send responses thread
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id None, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::Found decorated function: custom_input_formatter for decorator attribute: is_input_formatter
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::Found decorated function: custom_input_formatter for decorator attribute: is_input_formatter
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::Found decorated function: custom_output_formatter for decorator attribute: is_output_formatter
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::Found decorated function: custom_output_formatter for decorator attribute: is_output_formatter
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::Loaded formatters - input: <function custom_input_formatter at 0x779dfd513f60>, output: <function custom_output_formatter at 0x779dfd590860>
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::Construction vLLM engine args from the following DJL configs: {'model': '/tmp/.djl.ai/download/830936cbbc7c8d2253b4ebe798f7cfc0262b8919', 'tensor_parallel_size': 4, 'pipeline_parallel_size': 1, 'max_num_seqs': 32, 'dtype': 'auto', 'revision': None, 'max_loras': 4, 'enable_lora': False, 'trust_remote_code': False, 'cpu_offload_gb': 0, 'quantization': None}
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:23 [__init__.py:742] Resolved architecture: GPTNeoXForCausalLM
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:23 [__init__.py:1815] Using max model len 2048
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:24 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:28 [__init__.py:216] Automatically detected platform cuda.
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(EngineCore_DP0 pid=2061)[0;0m INFO 09-23 05:16:30 [core.py:654] Waiting for init message from front-end.
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(EngineCore_DP0 pid=2061)[0;0m INFO 09-23 05:16:30 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='/tmp/.djl.ai/download/830936cbbc7c8d2253b4ebe798f7cfc0262b8919', speculative_config=None, tokenizer='/tmp/.djl.ai/download/830936cbbc7c8d2253b4ebe798f7cfc0262b8919', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/tmp/.djl.ai/download/830936cbbc7c8d2253b4ebe798f7cfc0262b8919, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":64,"local_cache_dir":null}
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(EngineCore_DP0 pid=2061)[0;0m INFO 09-23 05:16:30 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_ea58db14'), local_subscribe_addr='ipc:///tmp/9d616e45-d43f-4180-bf5f-a546a7394f9b', remote_subscribe_addr=None, remote_addr_ipv6=False)
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:34 [__init__.py:216] Automatically detected platform cuda.
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:34 [__init__.py:216] Automatically detected platform cuda.
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:34 [__init__.py:216] Automatically detected platform cuda.
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:34 [__init__.py:216] Automatically detected platform cuda.
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: W0923 05:16:36.741000 2073 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: W0923 05:16:36.741000 2073 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: W0923 05:16:36.749000 2070 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: W0923 05:16:36.749000 2070 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: W0923 05:16:36.749000 2071 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: W0923 05:16:36.749000 2071 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: W0923 05:16:36.782000 2072 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: W0923 05:16:36.782000 2072 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: 2025-09-23 05:16:37,175 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: 2025-09-23 05:16:37,234 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: 2025-09-23 05:16:37,242 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: 2025-09-23 05:16:37,260 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9ad9fc69'), local_subscribe_addr='ipc:///tmp/8d86d95b-ad42-4327-8d85-631e1ad578ab', remote_subscribe_addr=None, remote_addr_ipv6=False)
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0c03b74b'), local_subscribe_addr='ipc:///tmp/bb976ca6-aa1a-41c5-89a1-4f58020074f8', remote_subscribe_addr=None, remote_addr_ipv6=False)
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_17e81b5a'), local_subscribe_addr='ipc:///tmp/13f0d09f-6115-47b0-aeb6-3dc1eabc9f4b', remote_subscribe_addr=None, remote_addr_ipv6=False)
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1e52376b'), local_subscribe_addr='ipc:///tmp/473efd1f-9650-4853-a299-3cbb3da5947e', remote_subscribe_addr=None, remote_addr_ipv6=False)
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [W923 05:16:39.330335751 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [W923 05:16:39.354665099 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [W923 05:16:39.356601639 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [W923 05:16:39.363435377 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:39 [__init__.py:1433] Found nccl from library libnccl.so.2
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:39 [__init__.py:1433] Found nccl from library libnccl.so.2
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:39 [pynccl.py:70] vLLM is using nccl==2.27.3
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:39 [pynccl.py:70] vLLM is using nccl==2.27.3
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:39 [__init__.py:1433] Found nccl from library libnccl.so.2
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:39 [pynccl.py:70] vLLM is using nccl==2.27.3
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:39 [__init__.py:1433] Found nccl from library libnccl.so.2
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:39 [pynccl.py:70] vLLM is using nccl==2.27.3
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:39 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:39 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:39 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:39 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_5ca31d26'), local_subscribe_addr='ipc:///tmp/e8308111-1052-42f0-97e7-c5a130754e3e', remote_subscribe_addr=None, remote_addr_ipv6=False)
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:39 [parallel_state.py:1165] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:39 [parallel_state.py:1165] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:39 [parallel_state.py:1165] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:39 [parallel_state.py:1165] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:39 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:39 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:39 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:16:39 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP2 pid=2072)[0;0m INFO 09-23 05:16:39 [gpu_model_runner.py:2338] Starting to load model /tmp/.djl.ai/download/830936cbbc7c8d2253b4ebe798f7cfc0262b8919...
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP1 pid=2071)[0;0m INFO 09-23 05:16:39 [gpu_model_runner.py:2338] Starting to load model /tmp/.djl.ai/download/830936cbbc7c8d2253b4ebe798f7cfc0262b8919...
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP3 pid=2073)[0;0m INFO 09-23 05:16:39 [gpu_model_runner.py:2338] Starting to load model /tmp/.djl.ai/download/830936cbbc7c8d2253b4ebe798f7cfc0262b8919...
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP0 pid=2070)[0;0m INFO 09-23 05:16:39 [gpu_model_runner.py:2338] Starting to load model /tmp/.djl.ai/download/830936cbbc7c8d2253b4ebe798f7cfc0262b8919...
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP2 pid=2072)[0;0m INFO 09-23 05:16:40 [gpu_model_runner.py:2370] Loading model from scratch...
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP3 pid=2073)[0;0m INFO 09-23 05:16:40 [gpu_model_runner.py:2370] Loading model from scratch...
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP1 pid=2071)[0;0m INFO 09-23 05:16:40 [gpu_model_runner.py:2370] Loading model from scratch...
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP0 pid=2070)[0;0m INFO 09-23 05:16:40 [gpu_model_runner.py:2370] Loading model from scratch...
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP2 pid=2072)[0;0m INFO 09-23 05:16:40 [cuda.py:362] Using Flash Attention backend on V1 engine.
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP3 pid=2073)[0;0m INFO 09-23 05:16:40 [cuda.py:362] Using Flash Attention backend on V1 engine.
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP1 pid=2071)[0;0m INFO 09-23 05:16:40 [cuda.py:362] Using Flash Attention backend on V1 engine.
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP0 pid=2070)[0;0m INFO 09-23 05:16:40 [cuda.py:362] Using Flash Attention backend on V1 engine.
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:   0% Completed | 0/46 [00:00<?, ?it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:   2% Completed | 1/46 [00:00<00:37,  1.21it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:   4% Completed | 2/46 [00:01<00:36,  1.22it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:   7% Completed | 3/46 [00:02<00:34,  1.23it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:   9% Completed | 4/46 [00:03<00:34,  1.23it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  11% Completed | 5/46 [00:04<00:33,  1.23it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  13% Completed | 6/46 [00:04<00:32,  1.23it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  15% Completed | 7/46 [00:05<00:31,  1.22it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  17% Completed | 8/46 [00:06<00:31,  1.22it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  20% Completed | 9/46 [00:06<00:25,  1.44it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  22% Completed | 10/46 [00:07<00:26,  1.35it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  24% Completed | 11/46 [00:08<00:26,  1.31it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  26% Completed | 12/46 [00:09<00:26,  1.28it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  28% Completed | 13/46 [00:10<00:26,  1.26it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  30% Completed | 14/46 [00:11<00:25,  1.25it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  33% Completed | 15/46 [00:11<00:24,  1.25it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  35% Completed | 16/46 [00:12<00:24,  1.24it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  37% Completed | 17/46 [00:13<00:23,  1.24it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  39% Completed | 18/46 [00:14<00:22,  1.23it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  41% Completed | 19/46 [00:15<00:21,  1.23it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  43% Completed | 20/46 [00:15<00:21,  1.23it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  46% Completed | 21/46 [00:16<00:20,  1.23it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  48% Completed | 22/46 [00:17<00:19,  1.24it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  50% Completed | 23/46 [00:18<00:18,  1.23it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  52% Completed | 24/46 [00:19<00:17,  1.23it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  54% Completed | 25/46 [00:20<00:17,  1.23it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  57% Completed | 26/46 [00:20<00:16,  1.23it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  59% Completed | 27/46 [00:21<00:15,  1.23it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  61% Completed | 28/46 [00:22<00:14,  1.23it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  63% Completed | 29/46 [00:23<00:13,  1.22it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  65% Completed | 30/46 [00:23<00:11,  1.40it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  67% Completed | 31/46 [00:24<00:11,  1.35it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  70% Completed | 32/46 [00:25<00:10,  1.31it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  72% Completed | 33/46 [00:26<00:10,  1.29it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  74% Completed | 34/46 [00:26<00:09,  1.27it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  76% Completed | 35/46 [00:27<00:08,  1.26it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  78% Completed | 36/46 [00:28<00:08,  1.25it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  80% Completed | 37/46 [00:29<00:07,  1.24it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  83% Completed | 38/46 [00:30<00:06,  1.24it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  85% Completed | 39/46 [00:31<00:05,  1.23it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  87% Completed | 40/46 [00:31<00:04,  1.23it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  89% Completed | 41/46 [00:32<00:04,  1.23it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  91% Completed | 42/46 [00:33<00:03,  1.23it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  93% Completed | 43/46 [00:34<00:02,  1.23it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  96% Completed | 44/46 [00:35<00:01,  1.23it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards:  98% Completed | 45/46 [00:35<00:00,  1.23it/s]
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP2 pid=2072)[0;0m INFO 09-23 05:17:16 [default_loader.py:268] Loading weights took 36.43 seconds
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP1 pid=2071)[0;0m INFO 09-23 05:17:16 [default_loader.py:268] Loading weights took 36.38 seconds
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP3 pid=2073)[0;0m INFO 09-23 05:17:16 [default_loader.py:268] Loading weights took 36.40 seconds
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards: 100% Completed | 46/46 [00:36<00:00,  1.23it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Loading pt checkpoint shards: 100% Completed | 46/46 [00:36<00:00,  1.25it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP0 pid=2070)[0;0m INFO 09-23 05:17:17 [default_loader.py:268] Loading weights took 36.78 seconds
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP1 pid=2071)[0;0m INFO 09-23 05:17:17 [gpu_model_runner.py:2392] Model loading took 9.5745 GiB and 36.624444 seconds
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP3 pid=2073)[0;0m INFO 09-23 05:17:17 [gpu_model_runner.py:2392] Model loading took 9.5745 GiB and 36.639319 seconds
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP2 pid=2072)[0;0m INFO 09-23 05:17:17 [gpu_model_runner.py:2392] Model loading took 9.5745 GiB and 36.669782 seconds
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP0 pid=2070)[0;0m INFO 09-23 05:17:17 [gpu_model_runner.py:2392] Model loading took 9.5745 GiB and 37.025132 seconds
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP2 pid=2072)[0;0m INFO 09-23 05:17:25 [backends.py:539] Using cache directory: /home/djl/.cache/vllm/torch_compile_cache/83c76f34cf/rank_2_0/backbone for vLLM's torch.compile
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP2 pid=2072)[0;0m INFO 09-23 05:17:25 [backends.py:550] Dynamo bytecode transform time: 7.23 s
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP1 pid=2071)[0;0m INFO 09-23 05:17:25 [backends.py:539] Using cache directory: /home/djl/.cache/vllm/torch_compile_cache/83c76f34cf/rank_1_0/backbone for vLLM's torch.compile
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP1 pid=2071)[0;0m INFO 09-23 05:17:25 [backends.py:550] Dynamo bytecode transform time: 7.25 s
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP3 pid=2073)[0;0m INFO 09-23 05:17:25 [backends.py:539] Using cache directory: /home/djl/.cache/vllm/torch_compile_cache/83c76f34cf/rank_3_0/backbone for vLLM's torch.compile
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP3 pid=2073)[0;0m INFO 09-23 05:17:25 [backends.py:550] Dynamo bytecode transform time: 7.33 s
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP0 pid=2070)[0;0m INFO 09-23 05:17:25 [backends.py:539] Using cache directory: /home/djl/.cache/vllm/torch_compile_cache/83c76f34cf/rank_0_0/backbone for vLLM's torch.compile
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP0 pid=2070)[0;0m INFO 09-23 05:17:25 [backends.py:550] Dynamo bytecode transform time: 7.40 s
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP0 pid=2070)[0;0m INFO 09-23 05:17:29 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.953 s
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP1 pid=2071)[0;0m INFO 09-23 05:17:29 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.034 s
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP2 pid=2072)[0;0m INFO 09-23 05:17:29 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.032 s
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP3 pid=2073)[0;0m INFO 09-23 05:17:29 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.047 s
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP0 pid=2070)[0;0m INFO 09-23 05:17:33 [monitor.py:34] torch.compile takes 7.40 s in total
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP1 pid=2071)[0;0m INFO 09-23 05:17:33 [monitor.py:34] torch.compile takes 7.25 s in total
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP3 pid=2073)[0;0m INFO 09-23 05:17:33 [monitor.py:34] torch.compile takes 7.33 s in total
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP2 pid=2072)[0;0m INFO 09-23 05:17:33 [monitor.py:34] torch.compile takes 7.23 s in total
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP1 pid=2071)[0;0m 2025-09-23 05:17:34,313 - INFO - flashinfer.jit: Loading JIT ops: sampling
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP3 pid=2073)[0;0m 2025-09-23 05:17:34,313 - INFO - flashinfer.jit: Loading JIT ops: sampling
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 2025-09-23 05:17:34,313 - INFO - flashinfer.jit: Loading JIT ops: sampling
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP1 pid=2071)[0;0m [rank1]:W0923 05:17:34.313000 2071 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP1 pid=2071)[0;0m [rank1]:W0923 05:17:34.313000 2071 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m [rank0]:W0923 05:17:34.313000 2070 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m [rank0]:W0923 05:17:34.313000 2070 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP3 pid=2073)[0;0m [rank3]:W0923 05:17:34.313000 2073 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP3 pid=2073)[0;0m [rank3]:W0923 05:17:34.313000 2073 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP2 pid=2072)[0;0m 2025-09-23 05:17:34,315 - INFO - flashinfer.jit: Loading JIT ops: sampling
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP2 pid=2072)[0;0m [rank2]:W0923 05:17:34.315000 2072 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP2 pid=2072)[0;0m [rank2]:W0923 05:17:34.315000 2072 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP1 pid=2071)[0;0m [rank1]:W0923 05:17:34.318000 2071 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP1 pid=2071)[0;0m [rank1]:W0923 05:17:34.318000 2071 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP1 pid=2071)[0;0m 2025-09-23 05:17:34,332 - INFO - flashinfer.jit: Finished loading JIT ops: sampling
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m [rank0]:W0923 05:17:34.368000 2070 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m [rank0]:W0923 05:17:34.368000 2070 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 2025-09-23 05:17:34,383 - INFO - flashinfer.jit: Finished loading JIT ops: sampling
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP3 pid=2073)[0;0m [rank3]:W0923 05:17:34.419000 2073 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP3 pid=2073)[0;0m [rank3]:W0923 05:17:34.419000 2073 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP3 pid=2073)[0;0m 2025-09-23 05:17:34,433 - INFO - flashinfer.jit: Finished loading JIT ops: sampling
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP2 pid=2072)[0;0m [rank2]:W0923 05:17:34.472000 2072 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP2 pid=2072)[0;0m [rank2]:W0923 05:17:34.472000 2072 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP2 pid=2072)[0;0m 2025-09-23 05:17:34,486 - INFO - flashinfer.jit: Finished loading JIT ops: sampling
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP0 pid=2070)[0;0m INFO 09-23 05:17:35 [gpu_worker.py:298] Available KV cache memory: 60.49 GiB
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP1 pid=2071)[0;0m INFO 09-23 05:17:35 [gpu_worker.py:298] Available KV cache memory: 60.49 GiB
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP3 pid=2073)[0;0m INFO 09-23 05:17:35 [gpu_worker.py:298] Available KV cache memory: 60.49 GiB
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP2 pid=2072)[0;0m INFO 09-23 05:17:35 [gpu_worker.py:298] Available KV cache memory: 60.49 GiB
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(EngineCore_DP0 pid=2061)[0;0m INFO 09-23 05:17:35 [kv_cache_utils.py:864] GPU KV cache size: 240,240 tokens
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(EngineCore_DP0 pid=2061)[0;0m INFO 09-23 05:17:35 [kv_cache_utils.py:868] Maximum concurrency for 2,048 tokens per request: 117.30x
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(EngineCore_DP0 pid=2061)[0;0m INFO 09-23 05:17:35 [kv_cache_utils.py:864] GPU KV cache size: 240,240 tokens
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(EngineCore_DP0 pid=2061)[0;0m INFO 09-23 05:17:35 [kv_cache_utils.py:868] Maximum concurrency for 2,048 tokens per request: 117.30x
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(EngineCore_DP0 pid=2061)[0;0m INFO 09-23 05:17:35 [kv_cache_utils.py:864] GPU KV cache size: 240,240 tokens
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(EngineCore_DP0 pid=2061)[0;0m INFO 09-23 05:17:35 [kv_cache_utils.py:868] Maximum concurrency for 2,048 tokens per request: 117.30x
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(EngineCore_DP0 pid=2061)[0;0m INFO 09-23 05:17:35 [kv_cache_utils.py:864] GPU KV cache size: 240,240 tokens
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(EngineCore_DP0 pid=2061)[0;0m INFO 09-23 05:17:35 [kv_cache_utils.py:868] Maximum concurrency for 2,048 tokens per request: 117.30x
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: [1;36m(Worker_TP0 pid=2070)[0;0m 
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|??        | 2/11 [00:00<00:00, 19.34it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|?????     | 5/11 [00:00<00:00, 20.36it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|????????  | 8/11 [00:00<00:00, 20.54it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|??????????| 11/11 [00:00<00:00, 19.65it/s]
[33mWARN [m [92mPyProcess[m W-2039-test-stderr: Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|??????????| 11/11 [00:00<00:00, 19.85it/s]
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP0 pid=2070)[0;0m INFO 09-23 05:17:36 [custom_all_reduce.py:203] Registering 979 cuda graph addresses
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP3 pid=2073)[0;0m INFO 09-23 05:17:36 [custom_all_reduce.py:203] Registering 979 cuda graph addresses
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP1 pid=2071)[0;0m INFO 09-23 05:17:36 [custom_all_reduce.py:203] Registering 979 cuda graph addresses
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP2 pid=2072)[0;0m INFO 09-23 05:17:36 [custom_all_reduce.py:203] Registering 979 cuda graph addresses
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP0 pid=2070)[0;0m INFO 09-23 05:17:37 [gpu_model_runner.py:3118] Graph capturing finished in 1 secs, took 0.15 GiB
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP0 pid=2070)[0;0m INFO 09-23 05:17:37 [gpu_worker.py:391] Free memory on device (78.76/79.25 GiB) on startup. Desired GPU memory utilization is (0.9, 71.33 GiB). Actual usage is 9.57 GiB for weight, 0.17 GiB for peak activation, 1.1 GiB for non-torch memory, and 0.15 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=64630016000` to fit into requested memory, or `--kv-cache-memory=72616560640` to fully utilize gpu memory. Current kv cache memory in use is 64946685952 bytes.
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP1 pid=2071)[0;0m INFO 09-23 05:17:37 [gpu_model_runner.py:3118] Graph capturing finished in 1 secs, took 0.15 GiB
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP1 pid=2071)[0;0m INFO 09-23 05:17:37 [gpu_worker.py:391] Free memory on device (78.76/79.25 GiB) on startup. Desired GPU memory utilization is (0.9, 71.33 GiB). Actual usage is 9.57 GiB for weight, 0.17 GiB for peak activation, 1.1 GiB for non-torch memory, and 0.15 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=64630016000` to fit into requested memory, or `--kv-cache-memory=72616560640` to fully utilize gpu memory. Current kv cache memory in use is 64946685952 bytes.
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP2 pid=2072)[0;0m INFO 09-23 05:17:37 [gpu_model_runner.py:3118] Graph capturing finished in 1 secs, took 0.15 GiB
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP2 pid=2072)[0;0m INFO 09-23 05:17:37 [gpu_worker.py:391] Free memory on device (78.76/79.25 GiB) on startup. Desired GPU memory utilization is (0.9, 71.33 GiB). Actual usage is 9.57 GiB for weight, 0.17 GiB for peak activation, 1.1 GiB for non-torch memory, and 0.15 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=64630016000` to fit into requested memory, or `--kv-cache-memory=72616560640` to fully utilize gpu memory. Current kv cache memory in use is 64946685952 bytes.
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP3 pid=2073)[0;0m INFO 09-23 05:17:37 [gpu_model_runner.py:3118] Graph capturing finished in 1 secs, took 0.15 GiB
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(Worker_TP3 pid=2073)[0;0m INFO 09-23 05:17:37 [gpu_worker.py:391] Free memory on device (78.76/79.25 GiB) on startup. Desired GPU memory utilization is (0.9, 71.33 GiB). Actual usage is 9.57 GiB for weight, 0.17 GiB for peak activation, 1.1 GiB for non-torch memory, and 0.15 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=64630016000` to fit into requested memory, or `--kv-cache-memory=72616560640` to fully utilize gpu memory. Current kv cache memory in use is 64946685952 bytes.
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: [1;36m(EngineCore_DP0 pid=2061)[0;0m INFO 09-23 05:17:37 [core.py:218] init engine (profile, create kv cache, warmup model) took 19.60 seconds
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:17:37 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 15015
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO 09-23 05:17:37 [async_llm.py:180] Torch profiler disabled. AsyncLLM CPU traces will not be collected.
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::vllm service initialized
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::empty response received from service.invoke_handler_async()
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m process is not ready
[32mINFO [m [92mPyProcess[m Model [test] initialized.
[32mINFO [m [92mWorkerThread[m Starting worker thread WT-0002 for model test (M-0001, READY) on device gpu(4)
[36mDEBUG[m [92mWorkerPool[m worker pool for model test (M-0001, READY):
 WT-0001-fixedPool
WT-0002-fixedPool

[32mINFO [m [92mModelServer[m Initialize BOTH server with: EpollServerSocketChannel.
[32mINFO [m [92mModelServer[m BOTH API bind to: http://0.0.0.0:8080
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId a116d243-db80-4783-a4eb-d0b289ea74c5, internal trackingId 92654d21-f61d-49fb-aa77-fee91e847ce4
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id 92654d21-f61d-49fb-aa77-fee91e847ce4, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604667
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [a116d243-db80-4783-a4eb-d0b289ea74c5] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 92654d21-f61d-49fb-aa77-fee91e847ce4
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 6788b00b-d692-407e-a932-5f267a259ad2, internal trackingId addecf08-35c0-42e0-87e7-b32e7a68ce35
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id addecf08-35c0-42e0-87e7-b32e7a68ce35, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604670
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [6788b00b-d692-407e-a932-5f267a259ad2] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId addecf08-35c0-42e0-87e7-b32e7a68ce35
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 2f78cdaf-3319-4dcb-b1da-82afdd4c3f32, internal trackingId cda12eab-d88a-4c9e-bee2-de3e32918833
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id cda12eab-d88a-4c9e-bee2-de3e32918833, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604673
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [2f78cdaf-3319-4dcb-b1da-82afdd4c3f32] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId cda12eab-d88a-4c9e-bee2-de3e32918833
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 8b1ab308-5d0c-493a-816f-2c5f57f15826, internal trackingId 42057c6a-4920-4187-b9f5-ff22d83b9e7e
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id 42057c6a-4920-4187-b9f5-ff22d83b9e7e, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604676
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [8b1ab308-5d0c-493a-816f-2c5f57f15826] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 42057c6a-4920-4187-b9f5-ff22d83b9e7e
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 11a9d1d3-5dc1-47db-a2ff-0b12c7875306, internal trackingId 2017ac5b-cc56-4d00-98cf-073c755a5c7a
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id 2017ac5b-cc56-4d00-98cf-073c755a5c7a, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604679
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [11a9d1d3-5dc1-47db-a2ff-0b12c7875306] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 2017ac5b-cc56-4d00-98cf-073c755a5c7a
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 1791aec5-ab75-4984-807c-616f5e83aac6, internal trackingId 2b3b300c-6b7c-4573-8801-c55ac87660fc
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id 2b3b300c-6b7c-4573-8801-c55ac87660fc, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604682
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [1791aec5-ab75-4984-807c-616f5e83aac6] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 2b3b300c-6b7c-4573-8801-c55ac87660fc
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId b4342f68-1457-4b9f-b685-46095a914543, internal trackingId 3015b2b1-735a-42c8-bbe3-f5bac194173f
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id 3015b2b1-735a-42c8-bbe3-f5bac194173f, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604683
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [b4342f68-1457-4b9f-b685-46095a914543] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 3015b2b1-735a-42c8-bbe3-f5bac194173f
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 5f226e49-86fa-448d-85f1-da54441a68c0, internal trackingId 4c0375ef-9808-48eb-961f-6bf5a5ecc935
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id 4c0375ef-9808-48eb-961f-6bf5a5ecc935, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604684
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [5f226e49-86fa-448d-85f1-da54441a68c0] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 4c0375ef-9808-48eb-961f-6bf5a5ecc935
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 03332fb3-d8ad-4828-8173-e6e480687e5f, internal trackingId c1f0909b-d76f-4f16-aea8-9d733e6da279
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id c1f0909b-d76f-4f16-aea8-9d733e6da279, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604687
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [03332fb3-d8ad-4828-8173-e6e480687e5f] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId c1f0909b-d76f-4f16-aea8-9d733e6da279
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId c1c2b82f-eef1-4873-94d4-157620edda78, internal trackingId e3603c65-c022-4d4c-b3c5-dcf49d1652a3
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id e3603c65-c022-4d4c-b3c5-dcf49d1652a3, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604690
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [c1c2b82f-eef1-4873-94d4-157620edda78] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId e3603c65-c022-4d4c-b3c5-dcf49d1652a3
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId e4a24fa9-cdbb-480c-b0bd-ab237f78a917, internal trackingId b53fec64-e84a-4233-9a27-29a73605ddc9
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id b53fec64-e84a-4233-9a27-29a73605ddc9, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604692
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [e4a24fa9-cdbb-480c-b0bd-ab237f78a917] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId b53fec64-e84a-4233-9a27-29a73605ddc9
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 35b87117-8adc-49ca-9288-d94ee6f63aef, internal trackingId cb35a80e-ec88-4fd4-8307-1c6957a10c02
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id cb35a80e-ec88-4fd4-8307-1c6957a10c02, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604695
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [35b87117-8adc-49ca-9288-d94ee6f63aef] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId cb35a80e-ec88-4fd4-8307-1c6957a10c02
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId a912c677-2ab7-4038-998a-eb087c5f85f1, internal trackingId d654e298-2d57-45a6-9de0-841a643cde95
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 959c8835-40d7-4121-b0db-a5975edd82b7, internal trackingId 33691b42-52f9-4f82-bc59-79d922f325d8
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 6443a027-5f15-4b47-bca1-20a1c46d311c, internal trackingId fb906c8b-607b-4fe4-bbb4-4f3d5bed387d
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id d654e298-2d57-45a6-9de0-841a643cde95, submitting to handler
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 3dd412ed-5d4e-4b66-9a59-4e5cb61ee11a, internal trackingId a62ba2cd-7bbc-4e4c-8020-8946277edb64
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id 33691b42-52f9-4f82-bc59-79d922f325d8, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id fb906c8b-607b-4fe4-bbb4-4f3d5bed387d, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id a62ba2cd-7bbc-4e4c-8020-8946277edb64, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604697
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [a912c677-2ab7-4038-998a-eb087c5f85f1] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId d654e298-2d57-45a6-9de0-841a643cde95
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 431753ad-d05c-408d-88d0-560e4b4b33db, internal trackingId e0540295-c381-4162-94a4-fdf05dec1331
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id e0540295-c381-4162-94a4-fdf05dec1331, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604698
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [959c8835-40d7-4121-b0db-a5975edd82b7] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 33691b42-52f9-4f82-bc59-79d922f325d8
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 77b5f4b9-6c1e-423f-b9b9-3647e92cc47f, internal trackingId 913c4d09-48a3-4576-8db0-00cd3914058b
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id 913c4d09-48a3-4576-8db0-00cd3914058b, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604699
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [6443a027-5f15-4b47-bca1-20a1c46d311c] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId fb906c8b-607b-4fe4-bbb4-4f3d5bed387d
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId e6169969-fe28-44a4-8c8a-96edcb1b62c1, internal trackingId 72fb2ba4-12bf-4c61-b963-32846e78763f
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id 72fb2ba4-12bf-4c61-b963-32846e78763f, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604699
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [3dd412ed-5d4e-4b66-9a59-4e5cb61ee11a] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId a62ba2cd-7bbc-4e4c-8020-8946277edb64
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId cf7b9597-d97e-45bc-aa62-34b9a40b1588, internal trackingId 270ee94d-6910-4aa2-b006-f48ecff8f72c
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id 270ee94d-6910-4aa2-b006-f48ecff8f72c, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604700
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [431753ad-d05c-408d-88d0-560e4b4b33db] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId e0540295-c381-4162-94a4-fdf05dec1331
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 4d8758f4-10c0-45b5-9dab-44e967fb6b89, internal trackingId 7611ee1a-b8de-4b8f-b1bf-a107c7201fea
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id 7611ee1a-b8de-4b8f-b1bf-a107c7201fea, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604700
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [77b5f4b9-6c1e-423f-b9b9-3647e92cc47f] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 913c4d09-48a3-4576-8db0-00cd3914058b
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId e26b1c80-28d3-4fdb-9597-9b4375099e49, internal trackingId b9b44e4a-4931-4f97-aad9-de162471dfa7
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id b9b44e4a-4931-4f97-aad9-de162471dfa7, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604701
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [4d8758f4-10c0-45b5-9dab-44e967fb6b89] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 7611ee1a-b8de-4b8f-b1bf-a107c7201fea
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId ccca9a0b-ff7c-4f38-bafc-5b55227c6bf8, internal trackingId 96d63448-fc84-4129-ad85-6e6418fb92a7
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id 96d63448-fc84-4129-ad85-6e6418fb92a7, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604701
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [e6169969-fe28-44a4-8c8a-96edcb1b62c1] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 72fb2ba4-12bf-4c61-b963-32846e78763f
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 0bd252a4-b5e7-4cfe-81d9-872fdee3c9f4, internal trackingId 99ca9a6f-b478-4f74-ae15-1fb102b8d7c9
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id 99ca9a6f-b478-4f74-ae15-1fb102b8d7c9, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604702
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [cf7b9597-d97e-45bc-aa62-34b9a40b1588] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 270ee94d-6910-4aa2-b006-f48ecff8f72c
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 87c196ea-0128-4b90-a762-ae4641be4130, internal trackingId c5c87c93-a904-4d3b-b4dd-9ae1320576b6
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id c5c87c93-a904-4d3b-b4dd-9ae1320576b6, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604702
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [87c196ea-0128-4b90-a762-ae4641be4130] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId c5c87c93-a904-4d3b-b4dd-9ae1320576b6
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 683f51c6-8120-4c58-aaf3-e8bd6b69edc2, internal trackingId e0f6c1da-2a2a-4ff0-95e3-05f171107026
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id e0f6c1da-2a2a-4ff0-95e3-05f171107026, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604703
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [e26b1c80-28d3-4fdb-9597-9b4375099e49] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId b9b44e4a-4931-4f97-aad9-de162471dfa7
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 30be8350-357a-4eb3-92dd-6431a3be190f, internal trackingId 8d663104-8a25-4fa3-a0c8-98393040edac
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id 8d663104-8a25-4fa3-a0c8-98393040edac, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604703
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [0bd252a4-b5e7-4cfe-81d9-872fdee3c9f4] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 99ca9a6f-b478-4f74-ae15-1fb102b8d7c9
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 405f832a-9bb9-43e5-90d8-6283a11dfa6e, internal trackingId 8a867675-c020-47f7-b85e-2776afda317b
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id 8a867675-c020-47f7-b85e-2776afda317b, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604704
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [ccca9a0b-ff7c-4f38-bafc-5b55227c6bf8] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 96d63448-fc84-4129-ad85-6e6418fb92a7
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 4c880654-daa4-46ab-964d-55ca6644d9ed, internal trackingId f2f24cd7-6b8a-4bfb-add5-f24b2204dc10
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id f2f24cd7-6b8a-4bfb-add5-f24b2204dc10, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604704
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [405f832a-9bb9-43e5-90d8-6283a11dfa6e] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 8a867675-c020-47f7-b85e-2776afda317b
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 2af3c914-1110-4c40-af39-8c34d011a4e6, internal trackingId bb66ec06-a4cf-4835-8a15-81b0ef6a7b52
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id bb66ec06-a4cf-4835-8a15-81b0ef6a7b52, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604704
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [30be8350-357a-4eb3-92dd-6431a3be190f] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 8d663104-8a25-4fa3-a0c8-98393040edac
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 7fede82d-650c-4498-955e-549023c12207, internal trackingId 6a24244b-3ecf-452e-a3df-dcad458f1e48
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id 6a24244b-3ecf-452e-a3df-dcad458f1e48, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604704
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [4c880654-daa4-46ab-964d-55ca6644d9ed] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId f2f24cd7-6b8a-4bfb-add5-f24b2204dc10
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 05ecca72-6476-4aee-9cc7-31d45218fe6e, internal trackingId c11388b6-9080-40c2-8848-250dc0741b60
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id c11388b6-9080-40c2-8848-250dc0741b60, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604705
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [683f51c6-8120-4c58-aaf3-e8bd6b69edc2] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId e0f6c1da-2a2a-4ff0-95e3-05f171107026
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604707
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [7fede82d-650c-4498-955e-549023c12207] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 6a24244b-3ecf-452e-a3df-dcad458f1e48
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604707
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [2af3c914-1110-4c40-af39-8c34d011a4e6] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId bb66ec06-a4cf-4835-8a15-81b0ef6a7b52
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604707
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [05ecca72-6476-4aee-9cc7-31d45218fe6e] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId c11388b6-9080-40c2-8848-250dc0741b60
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId c6da0221-134a-4507-b9ed-239dfd2edeae, internal trackingId fec3f97f-d731-4dde-bcc8-733a1f980353
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 8071d7f3-592a-46b9-8fdd-53f5c0b73ec5, internal trackingId 1d005d82-b17c-4c1f-80ff-70c6b7363381
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId fb3052d0-f23a-4a67-9a3a-998788c325ee, internal trackingId 7962f1c4-6330-415f-9670-bf185d64cd41
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId fa66ef5d-6c83-4e36-9db9-36b0137ea283, internal trackingId fb3a7be9-4038-4825-9a80-ab277503b274
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id fec3f97f-d731-4dde-bcc8-733a1f980353, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id 1d005d82-b17c-4c1f-80ff-70c6b7363381, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id 7962f1c4-6330-415f-9670-bf185d64cd41, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id fb3a7be9-4038-4825-9a80-ab277503b274, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604710
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604710
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604710
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [8071d7f3-592a-46b9-8fdd-53f5c0b73ec5] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 1d005d82-b17c-4c1f-80ff-70c6b7363381
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [c6da0221-134a-4507-b9ed-239dfd2edeae] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId fec3f97f-d731-4dde-bcc8-733a1f980353
[32mINFO [m [92mAsyncRequestManager[m Request [fb3052d0-f23a-4a67-9a3a-998788c325ee] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 7962f1c4-6330-415f-9670-bf185d64cd41
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604710
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [fa66ef5d-6c83-4e36-9db9-36b0137ea283] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId fb3a7be9-4038-4825-9a80-ab277503b274
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 860f16be-2fe7-43ba-b3ed-6a48b48e299f, internal trackingId 9047d4c0-c5ca-4e74-8ad0-9f5295f1092c
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id 9047d4c0-c5ca-4e74-8ad0-9f5295f1092c, submitting to handler
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId b80e62fb-ad55-4ec3-9b67-ec8596c2681f, internal trackingId 78c08d86-20c4-4722-bb24-91a0e1c3e199
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 307420b2-ac31-421b-a175-020d3acdc858, internal trackingId 88f9fc9d-0315-43e7-9028-5b4bde2ab98e
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 889a1027-a158-4d92-8d1b-5fd5d862107f, internal trackingId 20024a29-80f2-4679-882b-94587fe7c452
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id 78c08d86-20c4-4722-bb24-91a0e1c3e199, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id 88f9fc9d-0315-43e7-9028-5b4bde2ab98e, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id 20024a29-80f2-4679-882b-94587fe7c452, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604712
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [b80e62fb-ad55-4ec3-9b67-ec8596c2681f] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 78c08d86-20c4-4722-bb24-91a0e1c3e199
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 13b2516b-999d-40a4-9ebf-99729fb90a2c, internal trackingId 15b30d48-7b41-49b8-8b92-cc3d5899f8e3
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id 15b30d48-7b41-49b8-8b92-cc3d5899f8e3, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604713
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [307420b2-ac31-421b-a175-020d3acdc858] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 88f9fc9d-0315-43e7-9028-5b4bde2ab98e
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 42282b23-158d-480e-a2b3-71dc2d6ed5f7, internal trackingId 2ed8f273-b33e-41b8-a882-2db5bb551f24
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id 2ed8f273-b33e-41b8-a882-2db5bb551f24, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604713
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [860f16be-2fe7-43ba-b3ed-6a48b48e299f] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 9047d4c0-c5ca-4e74-8ad0-9f5295f1092c
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 3649f208-733c-4522-b8e4-ebc0449e68c6, internal trackingId d12e677f-d6d0-4358-be43-20c9fff13e15
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id d12e677f-d6d0-4358-be43-20c9fff13e15, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604713
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [889a1027-a158-4d92-8d1b-5fd5d862107f] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 20024a29-80f2-4679-882b-94587fe7c452
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId ccf658af-5b4e-4098-bb18-b755252f4d8a, internal trackingId a6625097-4829-4a30-9700-76a4b11475f6
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id a6625097-4829-4a30-9700-76a4b11475f6, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604714
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [13b2516b-999d-40a4-9ebf-99729fb90a2c] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 15b30d48-7b41-49b8-8b92-cc3d5899f8e3
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 4875d383-48ff-432a-a9cb-ffeab58a6fd2, internal trackingId a84773a9-e42f-4bc8-b437-4cecc0760e04
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id a84773a9-e42f-4bc8-b437-4cecc0760e04, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604716
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [42282b23-158d-480e-a2b3-71dc2d6ed5f7] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 2ed8f273-b33e-41b8-a882-2db5bb551f24
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId d8aa89a6-b48f-4129-8d99-966cfd717e60, internal trackingId 15de0438-652c-4955-ab36-3a11264e8a26
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id 15de0438-652c-4955-ab36-3a11264e8a26, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604716
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [3649f208-733c-4522-b8e4-ebc0449e68c6] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId d12e677f-d6d0-4358-be43-20c9fff13e15
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId ec5db3ee-8720-4468-9bdf-074a6dea7837, internal trackingId 3dbeb041-285c-4e21-ae4a-9cca3b83afc6
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id 3dbeb041-285c-4e21-ae4a-9cca3b83afc6, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604716
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [ccf658af-5b4e-4098-bb18-b755252f4d8a] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId a6625097-4829-4a30-9700-76a4b11475f6
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 7ab2a81d-51cb-4129-af67-25f5dcdca022, internal trackingId 61c28ccd-e248-4967-9747-efcddfe8df3b
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id 61c28ccd-e248-4967-9747-efcddfe8df3b, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604717
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [4875d383-48ff-432a-a9cb-ffeab58a6fd2] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId a84773a9-e42f-4bc8-b437-4cecc0760e04
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 45c6b9cb-5b3e-425d-a77e-969c1c78fb29, internal trackingId 7e75c911-2f36-4394-814f-5592fcd658e5
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id 7e75c911-2f36-4394-814f-5592fcd658e5, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604718
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [d8aa89a6-b48f-4129-8d99-966cfd717e60] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 15de0438-652c-4955-ab36-3a11264e8a26
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 3f3bfc2c-c279-4254-b556-7a0a9823ad9b, internal trackingId 9b5b2539-6391-41ef-a71c-55a540456236
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id 9b5b2539-6391-41ef-a71c-55a540456236, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604718
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [ec5db3ee-8720-4468-9bdf-074a6dea7837] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 3dbeb041-285c-4e21-ae4a-9cca3b83afc6
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 38d13052-fc65-470f-ae99-2da0b5010136, internal trackingId 962e5abe-667d-4aaf-b213-d3a9df62e01c
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::received new request with tracking_id 962e5abe-667d-4aaf-b213-d3a9df62e01c, submitting to handler
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604718
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [7ab2a81d-51cb-4129-af67-25f5dcdca022] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 61c28ccd-e248-4967-9747-efcddfe8df3b
[36mDEBUG[m [92mAsyncRequestManager[m Adding continuous batch request with external requestId 76c242bf-cc9b-4f3b-a353-a246baa471b7, internal trackingId b2919824-4b80-445f-96a5-830b6b9fb150
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::received new request with tracking_id b2919824-4b80-445f-96a5-830b6b9fb150, submitting to handler
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604719
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [45c6b9cb-5b3e-425d-a77e-969c1c78fb29] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 7e75c911-2f36-4394-814f-5592fcd658e5
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604721
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [3f3bfc2c-c279-4254-b556-7a0a9823ad9b] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 9b5b2539-6391-41ef-a71c-55a540456236
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604721
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-2039-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [38d13052-fc65-470f-ae99-2da0b5010136] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId 962e5abe-667d-4aaf-b213-d3a9df62e01c
[32mINFO [m [92mPyProcess[m W-262-test-stdout: INFO::CUSTOM_OUTPUT_FORMATTER_CALLED at 1758604721
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::putting result of inference to output queue
[32mINFO [m [92mPyProcess[m W-262-test-stdout: DEBUG::waiting for new inference response
[32mINFO [m [92mAsyncRequestManager[m Request [76c242bf-cc9b-4f3b-a353-a246baa471b7] completed
[36mDEBUG[m [92mAsyncRequestManager[m Removing request with trackingId b2919824-4b80-445f-96a5-830b6b9fb150
