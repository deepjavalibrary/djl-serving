region: "us-west-2"

cloudwatch:
  metrics_namespace: "SageMaker_LLM_Benchmark"

s3:
  bucket_name: "djl-benchmark"
  folder: "sm-trtllm"

metrics:
  timeToFirstToken_p50:
    metric_name: "TTFT_P50"
    unit: "Milliseconds"
    
  timeToFirstToken_p99:
    metric_name: "TTFT_P99"
    unit: "Milliseconds"
  
  intertokenLatency_p50:
    metric_name: "InterTokenLatency_P50"
    unit: "Milliseconds"
    
  intertokenLatency_p99:
    metric_name: "InterTokenLatency_P99"
    unit: "Milliseconds"
    
  costPerMillionInputTokens: 
    metric_name: "CostPerMillionInputTokens"
    unit: "Count"
    
  costPerMillionOutputTokens: 
    metric_name: "CostPerMillionOutputTokens"
    unit: "None"
  
  tokenizerFailed_Sum: 
    metric_name: "TokenizerErrorRate"
    unit: "Percent"

  numberOfInputTokens_p50:
    metric_name: "NumberOfInputTokens_p50"
    unit: "None"
      
  numberOfInputTokens_p99:
    metric_name: "NumberOfInputTokens_p99"
    unit: "None"

  numberOfOutputTokens_p50:
    metric_name: "NumberOfOutputTokens_p50"
    unit: "None"
      
  numberOfOutputTokens_p99:
    metric_name: "NumberOfOutputTokens_p99"
    unit: "None"

  clientInvocationErrors_Sum:
    metric_name: "ClientInvocationErrorRate"
    unit: "Percent"

  emptyInferenceResponse_Sum:
    metric_name: "EmptyInferenceResponseRate"
    unit: "Percent"

benchmarks:
  # Llama-3.1
  - model: "Llama-3.1-8b"
    endpoints: 
      - endpoint: "sagemaker"
        image: "TRTLLM"
        config: "benchmark_config_Llama-3-1-8b.json"
        dataset: "s3://djl-benchmark-datasets/openorca/openorca_trtllm_base_sample_payload_en_500-1000.tar.gz"
        action: yes 
  - model: "Llama-3.1-8b-instruct"
    endpoints: 
      - endpoint: "sagemaker"
        image: "TRTLLM-v12-8192-sample"
        config: "benchmark_config_Llama-3-1-8b-instruct.json"
        dataset: "s3://djl-benchmark-datasets/openorca/openorca_trtllm_instruct_sample_payload_en_500-1000.tar.gz"
        action: yes 
  - model: "Llama-3.1-70b"
    endpoints: 
      - endpoint: "sagemaker"
        image: "TRTLLM"
        config: "benchmark_config_Llama-3-1-70b.json"
        dataset: "s3://ddjl-benchmark-datasets/openorca/openorca_trtllm_base_sample_payload_en_500-1000.tar.gz"
        action: yes  
  - model: "Llama-3.1-70b-instruct"
    endpoints:  
      - endpoint: "sagemaker"
        image: "TRTLLM"
        config: "benchmark_config_Llama-3-1-70b-instruct.json"
        dataset: "s3://djl-benchmark-datasets/openorca/openorca_trtllm_instruct_sample_payload_en_500-1000.tar.gz"
        action: yes   
  - model: "Llama-3.1-405b-fp8"
    endpoints: 
      - endpoint: "sagemaker"
        image: "TRTLLM"
        config: "benchmark_config_Llama-3-1-405b-fp8.json"
        dataset: "s3://djl-benchmark-datasets/openorca/openorca_trtllm_base_sample_payload_en_500-1000.tar.gz"
        action: no
  - model: "Llama-3.1-405b-instruct-fp8"
    endpoints: 
      - endpoint: "sagemaker"
        image: "TRTLLM"
        config: "benchmark_config_Llama-3-1-405b-instruct-fp8.json"
        dataset: "s3://djl-benchmark-datasets/openorca/openorca_trtllm_instruct_sample_payload_en_500-1000.tar.gz"
        action: no 
