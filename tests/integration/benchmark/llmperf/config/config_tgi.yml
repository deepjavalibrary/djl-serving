region: "us-west-2"

cloudwatch:
  metrics_namespace: "EC2_LLM_Benchmark"

s3:
  #bucket_name: "djl-benchmark-llm"
  #folder: "ec2"
  bucket_name: "lninga-test"
  folder: "llmperf"

metrics:
  results_ttft_s_quantiles_p50:
    metric_name: "TTFT_P50"
    unit: "Milliseconds"
    
  results_ttft_s_quantiles_p99:
    metric_name: "TTFT_P99"
    unit: "Milliseconds"
  
  results_inter_token_latency_s_quantiles_p50:
    metric_name: "InterTokenLatency_P50"
    unit: "Milliseconds"
    
  results_inter_token_latency_s_quantiles_p99:
    metric_name: "InterTokenLatency_P99"
    unit: "Milliseconds"
    
  results_request_output_throughput_token_per_s_quantiles_p50: 
    metric_name: "OutputThroughputToken_P50"
    unit: "Count"
    
  results_request_output_throughput_token_per_s_quantiles_p99: 
    metric_name: "OutputThroughputToken_P99"
    unit: "None"

  results_number_input_tokens_quantiles_p50:
    metric_name: "NumberOfInputTokens_p50"
    unit: "None"
      
  results_number_input_tokens_quantiles_p99:
    metric_name: "NumberOfInputTokens_p99"
    unit: "None"

  results_number_output_tokens_quantiles_p50:
    metric_name: "NumberOfOutputTokens_p50"
    unit: "None"
      
  results_number_output_tokens_quantiles_p99:
    metric_name: "NumberOfOutputTokens_p99"
    unit: "None"

  results_error_rate:
    metric_name: "ErrorRate"
    unit: "Percent"

benchmarks:
  - model: "meta-llama/Llama-3.1-8B-Instruct"
    tests:
      - test_name: "tp2_500_250"
        llmperf_parameters:
          num-concurrent-requests-list: [1,2,4,8,16,32,64]
          others: ["--mean-input-tokens 500", "--stddev-input-tokens 150", "--mean-output-tokens 250", "--stddev-output-tokens 150", "--max-num-completed-requests 1000", "--timeout 600", "--llm-api openai"]
        containers:
          - container: "tgi"
            instance_types: ["p4d.24xlarge", "g5.12xlarge"]
            repo: "ghcr.io/huggingface/text-generation-inference"  
            tag: "latest"
            action: yes
            docker_parameters:
              ["--runtime", "nvidia", "--gpus", "all", "--shm-size", "20g", "-v",  "/home/ubuntu/.cache/huggingface:/data", "-p", "8080:80"]       
            server_parameters:
              # https://github.com/huggingface/text-generation-inference/blob/main/docs/source/reference/launcher.md?plain=1
              ["--model-id", "meta-llama/Llama-3.1-8B-Instruct", "--sharded", "false", "--max-input-length", "1024", "--max-total-tokens", "8192", "--max-best-of", "5", "--max-concurrent-requests", "5000", "--num-shard", "2", "--quantize", "fp8"]
