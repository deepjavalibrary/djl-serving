vllm req {'inputs': 'DeepSpeed is a machine learning framework', 'max_new_tokens': 256}
Running command TOKENIZER=TinyLlama/TinyLlama-1.1B-Chat-v1.0 ./awscurl -c 1 -N 100 -X POST http://127.0.0.1:8080/invocations --connect-timeout 300 -H 'Content-type: application/json' -d '{"inputs": "DeepSpeed is a machine learning framework", "max_new_tokens": 256}' --delay "rand(0,1000)" --json-path benchmark.json  -P -t
vllm-handler-batch-001
raw metrics: {'tokenizer': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'totalTimeMills': 13308.16, 'totalRequests': 100, 'failedRequests': 0, 'errorRate': 0.0, 'concurrentClients': 1, 'tps': 7.55, 'tokenThroughput': 233.91, 'totalTokens': 3098, 'tokenPerRequest': 30, 'averageLatency': 132.44, 'averageTokenLatency': 4.28, 'p50Latency': 122.87, 'p90Latency': 130.3, 'p99Latency': 696.79, 'timeToFirstByte': 129.42, 'p50TimeToFirstByte': 122.562964, 'p90TimeToFirstByte': 129.988471, 'p99TimeToFirstByte': 422.596974}
aws cloudwatch put-metric-data --namespace "serving_handler" --region "us-east-1" --metric-data '[{"MetricName": "vllm-handler-batch-001_p50Latency", "Unit": "Milliseconds", "Value": 122.87}, {"MetricName": "vllm-handler-batch-001_p90Latency", "Unit": "Milliseconds", "Value": 130.3}, {"MetricName": "vllm-handler-batch-001_p50TimeToFirstByte", "Unit": "Milliseconds", "Value": 122.562964}, {"MetricName": "vllm-handler-batch-001_p90TimeToFirstByte", "Unit": "Milliseconds", "Value": 129.988471}, {"MetricName": "vllm-handler-batch-001_tokenThroughput", "Unit": "Count/Second", "Value": 233.91}, {"MetricName": "vllm-handler-batch-001_tps", "Unit": "Count/Second", "Value": 7.55}, {"MetricName": "vllm-handler-batch-001_tokenPerRequest", "Unit": "Count", "Value": 30}]'
Running command TOKENIZER=TinyLlama/TinyLlama-1.1B-Chat-v1.0 ./awscurl -c 512 -N 10 -X POST http://127.0.0.1:8080/invocations --connect-timeout 300 -H 'Content-type: application/json' -d '{"inputs": "DeepSpeed is a machine learning framework", "max_new_tokens": 256}' --delay "rand(0,1000)" --json-path benchmark.json  -P -t
vllm-handler-batch-512
raw metrics: {'tokenizer': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'totalTimeMills': 10398.59, 'totalRequests': 5120, 'failedRequests': 0, 'errorRate': 0.0, 'concurrentClients': 512, 'tps': 525.69, 'tokenThroughput': 16230.34, 'totalTokens': 158077, 'tokenPerRequest': 30, 'averageLatency': 973.96, 'averageTokenLatency': 16151.48, 'p50Latency': 999.01, 'p90Latency': 1250.61, 'p99Latency': 1414.88, 'timeToFirstByte': 973.16, 'p50TimeToFirstByte': 998.768052, 'p90TimeToFirstByte': 1250.390846, 'p99TimeToFirstByte': 1414.618227}
aws cloudwatch put-metric-data --namespace "serving_handler" --region "us-east-1" --metric-data '[{"MetricName": "vllm-handler-batch-512_p50Latency", "Unit": "Milliseconds", "Value": 999.01}, {"MetricName": "vllm-handler-batch-512_p90Latency", "Unit": "Milliseconds", "Value": 1250.61}, {"MetricName": "vllm-handler-batch-512_p50TimeToFirstByte", "Unit": "Milliseconds", "Value": 998.768052}, {"MetricName": "vllm-handler-batch-512_p90TimeToFirstByte", "Unit": "Milliseconds", "Value": 1250.390846}, {"MetricName": "vllm-handler-batch-512_tokenThroughput", "Unit": "Count/Second", "Value": 16230.34}, {"MetricName": "vllm-handler-batch-512_tps", "Unit": "Count/Second", "Value": 525.69}, {"MetricName": "vllm-handler-batch-512_tokenPerRequest", "Unit": "Count", "Value": 30}]'
vllm-chat req {'messages': [{'role': 'user', 'content': 'hello, can you help me?'}, {'role': 'assistant', 'content': 'Hi, what can i help you with today?'}, {'role': 'user', 'content': 'What is deep learning?'}], 'max_tokens': 256}
Running command TOKENIZER=TinyLlama/TinyLlama-1.1B-Chat-v1.0 ./awscurl -c 1 -N 100 -X POST http://127.0.0.1:8080/invocations --connect-timeout 300 -H 'Content-type: application/json' -d '{"messages": [{"role": "user", "content": "hello, can you help me?"}, {"role": "assistant", "content": "Hi, what can i help you with today?"}, {"role": "user", "content": "What is deep learning?"}], "max_tokens": 256}' --delay "rand(0,1000)" --json-path benchmark.json -j "choices/message/content" -P -t
vllm-handler-chat-batch-001
raw metrics: {'tokenizer': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'totalTimeMills': 57000.08, 'totalRequests': 100, 'failedRequests': 0, 'errorRate': 0.0, 'concurrentClients': 1, 'tps': 1.78, 'tokenThroughput': 248.63, 'totalTokens': 13980, 'tokenPerRequest': 139, 'averageLatency': 562.27, 'averageTokenLatency': 4.02, 'p50Latency': 488.09, 'p90Latency': 953.29, 'p99Latency': 1752.68, 'timeToFirstByte': 558.72, 'p50TimeToFirstByte': 487.414934, 'p90TimeToFirstByte': 952.307327, 'p99TimeToFirstByte': 1473.524034}
aws cloudwatch put-metric-data --namespace "serving_handler" --region "us-east-1" --metric-data '[{"MetricName": "vllm-handler-chat-batch-001_p50Latency", "Unit": "Milliseconds", "Value": 488.09}, {"MetricName": "vllm-handler-chat-batch-001_p90Latency", "Unit": "Milliseconds", "Value": 953.29}, {"MetricName": "vllm-handler-chat-batch-001_p50TimeToFirstByte", "Unit": "Milliseconds", "Value": 487.414934}, {"MetricName": "vllm-handler-chat-batch-001_p90TimeToFirstByte", "Unit": "Milliseconds", "Value": 952.307327}, {"MetricName": "vllm-handler-chat-batch-001_tokenThroughput", "Unit": "Count/Second", "Value": 248.63}, {"MetricName": "vllm-handler-chat-batch-001_tps", "Unit": "Count/Second", "Value": 1.78}, {"MetricName": "vllm-handler-chat-batch-001_tokenPerRequest", "Unit": "Count", "Value": 139}]'
Running command TOKENIZER=TinyLlama/TinyLlama-1.1B-Chat-v1.0 ./awscurl -c 512 -N 10 -X POST http://127.0.0.1:8080/invocations --connect-timeout 300 -H 'Content-type: application/json' -d '{"messages": [{"role": "user", "content": "hello, can you help me?"}, {"role": "assistant", "content": "Hi, what can i help you with today?"}, {"role": "user", "content": "What is deep learning?"}], "max_tokens": 256}' --delay "rand(0,1000)" --json-path benchmark.json -j "choices/message/content" -P -t
vllm-handler-chat-batch-512
raw metrics: {'tokenizer': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'totalTimeMills': 43810.88, 'totalRequests': 5120, 'failedRequests': 0, 'errorRate': 0.0, 'concurrentClients': 512, 'tps': 121.25, 'tokenThroughput': 17565.71, 'totalTokens': 741715, 'tokenPerRequest': 144, 'averageLatency': 4222.52, 'averageTokenLatency': 14923.62, 'p50Latency': 3804.51, 'p90Latency': 6338.18, 'p99Latency': 7320.74, 'timeToFirstByte': 4221.45, 'p50TimeToFirstByte': 3803.952391, 'p90TimeToFirstByte': 6337.128306, 'p99TimeToFirstByte': 7319.608034}
aws cloudwatch put-metric-data --namespace "serving_handler" --region "us-east-1" --metric-data '[{"MetricName": "vllm-handler-chat-batch-512_p50Latency", "Unit": "Milliseconds", "Value": 3804.51}, {"MetricName": "vllm-handler-chat-batch-512_p90Latency", "Unit": "Milliseconds", "Value": 6338.18}, {"MetricName": "vllm-handler-chat-batch-512_p50TimeToFirstByte", "Unit": "Milliseconds", "Value": 3803.952391}, {"MetricName": "vllm-handler-chat-batch-512_p90TimeToFirstByte", "Unit": "Milliseconds", "Value": 6337.128306}, {"MetricName": "vllm-handler-chat-batch-512_tokenThroughput", "Unit": "Count/Second", "Value": 17565.71}, {"MetricName": "vllm-handler-chat-batch-512_tps", "Unit": "Count/Second", "Value": 121.25}, {"MetricName": "vllm-handler-chat-batch-512_tokenPerRequest", "Unit": "Count", "Value": 144}]'
