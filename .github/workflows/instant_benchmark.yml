name: instant benchmark tooling

on:
  workflow_dispatch:
    inputs:
      running_template:
        description: 'A json file that contains benchmark plans'
        required: true
      instance:
        description: 'Instance used for benchmark'
        required: true
        default: 'g5.12xlarge'
        type: choice
        options:
          - g5.2xlarge
          - g5.12xlarge
          - g5.48xlarge
          - g6.2xlarge
          - g6.12xlarge
          - g6.48xlarge
          - g4dn.12xlarge
          - g4dn.2xlarge
          - p4d.24xlarge
          - p4de.24xlarge
          - p5.24xlarge
          - inf2.8xlarge
          - inf2.24xlarge
          - trn1.2xlarge
          - trn1.32xlarge
      container:
        description: 'The container used to run benchmark (overrides the template). Should be a full docker path such as deepjavalibrary/djl-serving:0.28.0-lmi'
        required: false
        default: ''
      record:
        description: 'Whether to record the results'
        default: 'none'
        type: choice
        options:
          - none
          - table
          - cloudwatch
      repo:
        description: '[Do not change] The repo for runner registration'
        required: false
        type: string
        default: 'djl-serving'
  workflow_call:
    inputs:
      running_template:
        description: 'A json file that contains benchmark plans'
        required: true
        type: string
      instance:
        description: 'Instance used for benchmark'
        required: true
        type: string
      container:
        description: 'The container used to run benchmark (overrides the template). Should be a full docker path such as deepjavalibrary/djl-serving:0.27.0-deepspeed'
        required: false
        type: string
        default: ''
      record:
        description: 'Whether to record the results'
        required: false
        type: string
        default: 'none'
      repo:
        description: 'The repo for runner registration'
        required: false
        type: string
        default: 'djl-serving'

permissions:
  id-token: write
  contents: read

jobs:
  create-runners:
    runs-on: [self-hosted, scheduler]
    steps:
      - name: Create new instance
        id: create_instance
        run: |
          cd /home/ubuntu/djl_benchmark_script/scripts
          token=$( curl -X POST -H "Authorization: token ${{ secrets.ACTION_RUNNER_PERSONAL_TOKEN }}" \
          https://api.github.com/repos/deepjavalibrary/${{ inputs.repo }}/actions/runners/registration-token \
          --fail \
          | jq '.token' | tr -d '"' )
          ./start_instance.sh action_ib_${{ inputs.instance }} $token ${{ inputs.repo }}
    outputs:
      gpu_instance_id: ${{ steps.create_instance.outputs.action_ib_instance_id }}

  environment-setup:
    runs-on: [ self-hosted, "${{ inputs.instance }}" ]
    timeout-minutes: 15
    needs: [ create-runners ]
    steps:
      - uses: actions/checkout@v4
        if: ${{ inputs.repo == 'djl-serving' }}
      - name: Setup DJLServing for other repo
        uses: actions/checkout@v4
        if: ${{ inputs.repo != 'djl-serving' }}
        with:
          repository: deepjavalibrary/djl-serving
          ref: master
      - name: Setup for other repo
        uses: actions/checkout@v4
        if: ${{ inputs.repo != 'djl-serving' }}
        with:
          path: ${{ inputs.repo }}
      - name: Set up Python3
        uses: actions/setup-python@v5
        with:
          python-version: '3.10.x'
      - name: Clean env
        run: |
          yes | docker system prune -a --volumes
      - name: install deps
        run: |
          pip3 install boto3 awscli
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::185921645874:role/github-actions-djl-serving
          aws-region: us-east-1
      - name: Parse job schema
        working-directory: tests/integration
        id: generate_matrix
        run: |
          python3 instant_benchmark.py --parse ${{ inputs.running_template }} \
          --container "${{ inputs.container }}"
    outputs:
      jobs: ${{ steps.generate_matrix.outputs.jobs }}
      template: ${{ steps.generate_matrix.outputs.template }}

  benchmark_run:
    runs-on: [ self-hosted, "${{ inputs.instance }}" ]
    timeout-minutes: 90
    needs: [ environment-setup ]
    strategy:
      matrix:
        job: ${{ fromJSON(needs.environment-setup.outputs.jobs) }}
    steps:
      - uses: actions/checkout@v4
        if: ${{ inputs.repo == 'djl-serving' }}
      - name: Setup DJLServing for other repo
        uses: actions/checkout@v4
        if: ${{ inputs.repo != 'djl-serving' }}
        with:
          repository: deepjavalibrary/djl-serving
          ref: master
      - name: Setup for other repo
        uses: actions/checkout@v4
        if: ${{ inputs.repo != 'djl-serving' }}
        with:
          path: ${{ inputs.repo }}
      - name: Set up Python3
        uses: actions/setup-python@v5
        with:
          python-version: '3.10.x'
      - name: install deps
        run: |
          pip3 install boto3 awscli
      - name: Setup awscurl
        working-directory: tests/integration
        run: |
          wget https://publish.djl.ai/awscurl/awscurl
          chmod +x awscurl
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::185921645874:role/github-actions-djl-serving
          aws-region: us-east-1
      - name: Run benchmark job
        working-directory: tests/integration
        run: |
          echo "${{ needs.environment-setup.outputs.template }}" >> template.json
          python3 instant_benchmark.py --template template.json \
          --job ${{ matrix.job }} --instance ${{ inputs.instance }} \
          --record ${{ inputs.record }}
          
          bash instant_benchmark.sh
      - name: Get serving logs
        if: always()
        working-directory: tests/integration
        run: |
          ./remove_container.sh || true
          cat logs/serving.log || true
      - name: Upload test artifacts
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: ${{ matrix.job }}
          path: tests/integration

  stop-runners:
    if: always()
    runs-on: [ self-hosted, scheduler ]
    needs: [ create-runners, environment-setup, benchmark_run ]
    steps:
      - name: Stop instances
        run: |
          cd /home/ubuntu/djl_benchmark_script/scripts
          instance_id=${{ needs.create-runners.outputs.gpu_instance_id }}
          ./stop_instance.sh $instance_id
