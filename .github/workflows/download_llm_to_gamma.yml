name: Download the LLM to LMI Gamma account

on:
  workflow_dispatch:
    inputs:
      model_id:
        description: 'The hf model id of your model'
        required: true
      s3_model_name:
        description: 'The name used in s3'
        required: true
      token:
        description: 'The hf token used to download'
        required: false
      allow_pattern:
        description: 'The list of file postfix to download'
        required: false
        default: '*.json, *.pt, *.safetensors, *.txt, *.model, *.tiktoken'


jobs:
  create-cpu-runner:
    runs-on: [ self-hosted, scheduler ]
    steps:
      - name: Create new CPU instance
        id: create_cpu
        run: |
          cd /home/ubuntu/djl_benchmark_script/scripts
          token=$( curl -X POST -H "Authorization: token ${{ secrets.ACTION_RUNNER_PERSONAL_TOKEN }}" \
          https://api.github.com/repos/deepjavalibrary/djl-serving/actions/runners/registration-token \
          --fail \
          | jq '.token' | tr -d '"' )
          ./start_instance.sh action_cpu $token djl-serving
    outputs:
      action_cpu_instance_id: ${{ steps.create_cpu.outputs.action_cpu_instance_id }}

  download_job:
    runs-on:
      - self-hosted
      - cpu
      - RUN_ID-${{ github.run_id }}
      - RUN_NUMBER-${{ github.run_number }}
      - SHA-${{ github.sha }}
    timeout-minutes: 60
    needs: create-cpu-runner
    steps:
      - uses: actions/checkout@v4
      - name: Clean docker env
        working-directory: serving/docker
        run: |
          yes | docker system prune -a --volumes
      - name: Set up Python3
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install pip dependencies
        run: pip3 install awscli huggingface_hub
      - name: Download the model to local
        working-directory: tests/integration/llm
        run: |
          python3 download_llm.py ${{ inputs.model_id }} --token "${{ inputs.token }}" --allow-patterns "${{ inputs.allow_pattern }}"
          rm -rf model/.huggingface/
      - name: upload the model to s3
        working-directory: tests/integration/llm
        run: |
          aws s3 sync model/ s3://djl-llm/${{ inputs.s3_model_name }}/
          rm -rf model/
      - name: Clean docker env
        working-directory: serving/docker
        run: |
          yes | docker system prune -a --volumes

  stop-runner:
    if: always()
    runs-on: [ self-hosted, scheduler ]
    needs: [download_job, create-cpu-runner]
    steps:
      - name: Stop all instances
        run: |
          cd /home/ubuntu/djl_benchmark_script/scripts
          instance_id=${{ needs.create-cpu-runner.outputs.action_cpu_instance_id }}
          ./stop_instance.sh $instance_id
