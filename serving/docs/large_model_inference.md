# Large model inference

DJLServing has the capability to host large language models and foundation models that does not fit into a single GPU. \
We maintain a collection of deep learning containers (DLC) specifically designed for conducting inference with large models on SageMaker.
You can explore the available deep learning containers [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers). 
The [AWS DLC for LMI](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-dlc.html) provides documentation detailing the description of libraries available for use with these DLCs.

To learn more about Large Model Inference with DJLServing on SageMaker, please see our dedicated docs [here](lmi/README.md).

